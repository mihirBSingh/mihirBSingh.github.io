[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Mihir Singh"
  },
  {
    "objectID": "posts/implementing-perceptron/index.html",
    "href": "posts/implementing-perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Mihir Singh\n\n\nThis blog posts first implements a perceptron algorithm and a minibatch perceptron algorithm. For the perceptron algorithm, experiments were run to check how it performed on linearly separable data, non-linearly separable data, and data with more than 2 dimensions. Experiments with the minibatch perceptron algorithm first focused on its performance compared to the regular perceptron algorithm, its performance on data with multiple dimensions, and its ability to converge when looking at all the data in a single iteration. Through this process, I developed an understanding of the perceptron algorithm and how it can be improved upon with a minibatch algorithm.\n\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\nThe code for the perceptron is here.\nFor our experiments we will use data from the in-class perceptron warmup which we know is linearly separable.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nFrom this we get a visual confirmation that our data is linearly separable. We should first check if our perceptron probably works by running it on our minimal training loop. We know it works if our perceptron converges and we achieve a loss of 0.\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # look at a single data point - consulted with Sophie Seiple to get single point  \n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n    \n\ntensor(0.)\n\n\nWe see that our tensor eventually becomes 0, so we are indeed converging on 0 and our perceptron is probably correct.\n\n\n\nNow we can experiment with our perceptron and look at when the data is linearly separable, not linearly separable, and look at how the perceptron works in 2 dimensions.\n\n\nWe can use the same data from the example above (since we know that the data is linearly separable) and visualize the process of a perceptron finding a line the separates the data.\n\n# Code below provided by Professor Phil Chodrow\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# functions for plotting perceptron data\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nJust to be sure, let’s look at the data first to confirm it looks linearly separable visually.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X,y, ax)\n\n\n\n\n\n\n\n\nIndeed, our data looks linearly separable. Now let’s run our perceptron.\n\n# Parts of code for visualization provided by Professor Phil Chodrow\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n\nwhile loss &gt; 0:\n    # we are always still look at a single data point\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    \n    ax = axarr.ravel()[current_ax]\n    \n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value     \n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n        \nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe can see that our perceptron becomes more accurate, converging on the correct answer. Another way to visualize our perceptron’s accuracy is through a loss graph.\n\n# Loss graph\ndef loss_graph(loss_vec):\n    plt.plot(loss_vec, marker='o', linestyle='-')\n    plt.title('Loss Graph: Experiment 1')\n    plt.xlabel('Step')\n    plt.ylabel('Loss')\n    plt.show()\n    print(\"final loss: \", loss_vec[len(loss_vec)-1])\n    \nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.0\n\n\nWe see that eventually (after may steps) we eventually do reach a loss of 0., but our loss graph shows the accuracy of the perceptron for every step over time.\nLet’s see what the decision boundary looks like on the final iteration.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nWe see that on the final step, the perceptron settles on a line that accurately separated all the data.\n\n\n\nWe’ve looked at the perceptron’s accuracy when we know that the data is linearly separable. Now we can look at how a perceptron will perform when we know that the data is not linearly separable.\nTo do this, we first need to create data that we know is not linearly separable. We can use the same perceptron_data function from before, but we can instead set it so that the noise is higher. This has the effect of points from the different classes crossing each other’s boundaries and thus making the data not linearly separable.\n\nX, y = perceptron_data(noise=0.5)\ny_ = (y+1)/2\n\nLet’s also plot the training and testing data for a visual inspection.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X,y, ax)\n\n\n\n\n\n\n\n\nNow we can visually see that out data is not perfectly linearly separable.\nLet’s begin training our perceptron. We will loop through for 1000 iterations. Let’s see what we come up with after 500 and then finally when we finish at 1000 iterations\n\n# Initialize perceptron\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\n# Training loop to 500\niterations = 0\nwhile iterations &lt; 500:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    iterations += 1\n    loss_vec.append(loss)\n    \n# Plot perceptron after 500 iterations\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nprint(loss)\n\n0.1666666716337204\n\n\n\n\n\n\n\n\n\nIt is somewhat accurate, but it’s a stretch to say perfect like in experiment 1. Let’s go to 1000 iterations. For the final iteration, we will also plot the decision boundary of the perceptron before we stopped iterating.\n\n# Training loop to 1000\niterations = 0\nloss_vec = []\n\nwhile iterations &lt; 1000:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    iterations += 1\n    loss_vec.append(loss)\n    \n# Plot perceptron after 500 iterations (with decision boundaries)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nprint(\"loss:\", loss)\n\nloss: 0.1066666692495346\n\n\n\n\n\n\n\n\n\nWe see here that our loss decreases slightly. Let’s take a look at the loss over time.\n\nprint(len(loss_vec))\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 2')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"final loss: \", loss_vec[len(loss_vec)-1])\n\n1000\nfinal loss:  0.1066666692495346\n\n\n\n\n\n\n\n\n\nWe can see that we can’t say the loss generally decreases and converges on an answer - like we could generalize in part 1.\n\n\n\nNow we will look at the perceptron algorithm working in more than two dimensions. To do this, we can run our perceptron algorithm on data with 5 features.\nFirst we need to generate perceptron data with 5 features. To do this we need to modify the perceptron_data function that we have been using. We’ll also make the noise 0.35 (which is just the average of the noises from the pervious experiments)\n\ndef multi_dim_perceptron_data(n_points = 300, noise = 0.35, dim=5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, dim))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = multi_dim_perceptron_data(dim=5)\n\nWe can look at the score of our perceptron over the training period to help us answer if the data is linearly separable. First we need to run our algorithm.\n\n# rerun perceptron for experiment 3\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\nloss = 1\nloss_vec = []\nscores = []\nwhile loss &gt; 0:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(xi, yi)\n    scores.append( p.score(X))\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nNow to check if the data is linearly separable, we need to check if all the dot products between the weight vector w and feature vector xi are the same signs. This is because if the score and the target vector have opposite signs then the perceptron needs to update and we have not found a perfectly correct answer.\n\nclassification = p.score(X)*y\nclassification.min()\n\ntensor(0.3203)\n\n\nSince our minimum value in classification is 0.0`68 we know that for any value of i, &lt;w,xi&gt; &gt; 0 if y = 1, and thus we have a linearly separable dataset.\nTo be extra sure we can also look at the loss and see if the perceptron converges such that it comes to the right answer and has a loss of 0.\n\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 3')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"final loss: \", loss_vec[len(loss_vec)-1])\n\n\n\n\n\n\n\n\nfinal loss:  0.0\n\n\nWe see that our loss does indeed eventually go to 0 further indicating our perceptron settled on a hyperplane that perfectly separates the data.\nTherefore, the dataset in this experiment is linearly separable\n\n\n\n\nNow we can look at how a minibatch perceptron performs. Instead of looking at a single point during each iteration, we will look at k randomly selected points. In addition to this change we add a learning rate alpha that changes how much the weights change with each iteration.\nTo do this we should update our grad function. The code for the grad function and the whole minibatch perceptron function is here.\nWith our implmentation of the minibatch perceptron we can canduct a few experiments.\n\n\nWe should first check to see that our minibatch perceptron performs similarly to a regular perceptron when it is only looking at one point (just like a regular perceptron). In this case, k = 1.\nWe will first look at how a minibatch perceptron performs compared to a regular perceptron when the data is linearly separable. To do this we will see if the minibatch perceptron converges to a loss of 0 and look at its loss graph.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nperceptron_data(n_points = 300, noise = 0.2)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n\nloss_graph(loss_vec=loss_vec)\n\ntensor(0.)\nfinal loss:  tensor(0.)\n\n\n\n\n\n\n\n\n\nIt takes more steps, but we do find that the perceptron converges to a loss of 0.\nNow let’s look at how the minibatch perceptron performs with overlapping data.\n\nX, y = perceptron_data(noise=0.5)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\niterations = 0\nwhile iterations &lt; 1000: \n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n        \n    iterations += 1\n\nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.08666666597127914\n\n\nAfter 1000 iterations we see that just like the regular perceptron, the minibatch perceptron struggles a little bit but still finds a hyperplane with a low loss.\n\n\n\n\nNow looking at 5 dimensional data where k=10, we should also find that the minibatch perceptron finds a separating line.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nX, y = multi_dim_perceptron_data(n_points = 300, noise = 0.2, dim=5)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n\nloss_graph(loss_vec=loss_vec)\n\ntensor(0.)\nfinal loss:  tensor(0.)\n\n\n\n\n\n\n\n\n\nAs expected, the minibatch perceptron does indeed find a separating line.\n\n\nNow what happens if k = n, that is we check the entire data set at once. Even if the data is not linearly separable, our minibatch perceptron should be able to converge.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nX, y = perceptron_data(n_points = 300, noise = 0.5)\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\niterations = 0\nwhile iterations &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = n\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n        \n    iterations += 1\n\nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.07666666805744171\n\n\nWe see that our loss converges to around 0.07, which is pretty small, even though the data is not linearly separable.\n\n\n\n\nThe runtime complexity of a single iteration is O(N) where N is the number of features (the size of a single row in the feature matrix) This is because the number of computations that occur in a single iteration depends on the dot product between w and Xi where X is the feature matrix and i is a single row in the matrix.\nThe minibatch is essentially the same but instead of looking at just one row of the feature matrix, you are looking at k rows in a single iteration. So the runtime becomes O(KN) where K is the number of rows you are looking at in a single iteration.\nPerceptron grad works by computing the score for a single data point (the dot product between the weight and all the features of the data point). If the score multiplied by the target is below 0 then we have misclassified and our weight is not perfectly accurate. If this is true then we want to multiply Xi * yi to get how much we should change the weight by for each feature (we end up adding this value to our weight in our step function). If we don’t misclassify then we are fine and we return a vector of 0s so we don’t change our weight by anything in the step function.\n\n\n\nThe perceptron algorithm works by continuously updating its weight according to on how accurately it classified the data points given to it. Implementing the perceptron and minibatch perceptron gave me a solid foundational understanding of the perceptron’s design as well as situations where it is well suited to separating data and situations where it is disadvantaged. The experiments helped me further understand the perceptron’s performance over linearly separable data, non-linearly separable data, and multi-dimensional data. Further experimentation with the minibatch perceptron helped me understand how it can converge when a regular perceptron can’t like when it looks at all the data points in a single iteration."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#abstract",
    "href": "posts/implementing-perceptron/index.html#abstract",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "This blog posts first implements a perceptron algorithm and a minibatch perceptron algorithm. For the perceptron algorithm, experiments were run to check how it performed on linearly separable data, non-linearly separable data, and data with more than 2 dimensions. Experiments with the minibatch perceptron algorithm first focused on its performance compared to the regular perceptron algorithm, its performance on data with multiple dimensions, and its ability to converge when looking at all the data in a single iteration. Through this process, I developed an understanding of the perceptron algorithm and how it can be improved upon with a minibatch algorithm.\n\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#implementation",
    "href": "posts/implementing-perceptron/index.html#implementation",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The code for the perceptron is here.\nFor our experiments we will use data from the in-class perceptron warmup which we know is linearly separable.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nFrom this we get a visual confirmation that our data is linearly separable. We should first check if our perceptron probably works by running it on our minimal training loop. We know it works if our perceptron converges and we achieve a loss of 0.\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # look at a single data point - consulted with Sophie Seiple to get single point  \n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n    \n\ntensor(0.)\n\n\nWe see that our tensor eventually becomes 0, so we are indeed converging on 0 and our perceptron is probably correct."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#part-b-experiments",
    "href": "posts/implementing-perceptron/index.html#part-b-experiments",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Now we can experiment with our perceptron and look at when the data is linearly separable, not linearly separable, and look at how the perceptron works in 2 dimensions.\n\n\nWe can use the same data from the example above (since we know that the data is linearly separable) and visualize the process of a perceptron finding a line the separates the data.\n\n# Code below provided by Professor Phil Chodrow\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# functions for plotting perceptron data\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nJust to be sure, let’s look at the data first to confirm it looks linearly separable visually.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X,y, ax)\n\n\n\n\n\n\n\n\nIndeed, our data looks linearly separable. Now let’s run our perceptron.\n\n# Parts of code for visualization provided by Professor Phil Chodrow\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n\nwhile loss &gt; 0:\n    # we are always still look at a single data point\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    \n    ax = axarr.ravel()[current_ax]\n    \n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value     \n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n        \nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe can see that our perceptron becomes more accurate, converging on the correct answer. Another way to visualize our perceptron’s accuracy is through a loss graph.\n\n# Loss graph\ndef loss_graph(loss_vec):\n    plt.plot(loss_vec, marker='o', linestyle='-')\n    plt.title('Loss Graph: Experiment 1')\n    plt.xlabel('Step')\n    plt.ylabel('Loss')\n    plt.show()\n    print(\"final loss: \", loss_vec[len(loss_vec)-1])\n    \nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.0\n\n\nWe see that eventually (after may steps) we eventually do reach a loss of 0., but our loss graph shows the accuracy of the perceptron for every step over time.\nLet’s see what the decision boundary looks like on the final iteration.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nWe see that on the final step, the perceptron settles on a line that accurately separated all the data.\n\n\n\nWe’ve looked at the perceptron’s accuracy when we know that the data is linearly separable. Now we can look at how a perceptron will perform when we know that the data is not linearly separable.\nTo do this, we first need to create data that we know is not linearly separable. We can use the same perceptron_data function from before, but we can instead set it so that the noise is higher. This has the effect of points from the different classes crossing each other’s boundaries and thus making the data not linearly separable.\n\nX, y = perceptron_data(noise=0.5)\ny_ = (y+1)/2\n\nLet’s also plot the training and testing data for a visual inspection.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X,y, ax)\n\n\n\n\n\n\n\n\nNow we can visually see that out data is not perfectly linearly separable.\nLet’s begin training our perceptron. We will loop through for 1000 iterations. Let’s see what we come up with after 500 and then finally when we finish at 1000 iterations\n\n# Initialize perceptron\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\n# Training loop to 500\niterations = 0\nwhile iterations &lt; 500:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    iterations += 1\n    loss_vec.append(loss)\n    \n# Plot perceptron after 500 iterations\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nprint(loss)\n\n0.1666666716337204\n\n\n\n\n\n\n\n\n\nIt is somewhat accurate, but it’s a stretch to say perfect like in experiment 1. Let’s go to 1000 iterations. For the final iteration, we will also plot the decision boundary of the perceptron before we stopped iterating.\n\n# Training loop to 1000\niterations = 0\nloss_vec = []\n\nwhile iterations &lt; 1000:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    iterations += 1\n    loss_vec.append(loss)\n    \n# Plot perceptron after 500 iterations (with decision boundaries)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nprint(\"loss:\", loss)\n\nloss: 0.1066666692495346\n\n\n\n\n\n\n\n\n\nWe see here that our loss decreases slightly. Let’s take a look at the loss over time.\n\nprint(len(loss_vec))\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 2')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"final loss: \", loss_vec[len(loss_vec)-1])\n\n1000\nfinal loss:  0.1066666692495346\n\n\n\n\n\n\n\n\n\nWe can see that we can’t say the loss generally decreases and converges on an answer - like we could generalize in part 1.\n\n\n\nNow we will look at the perceptron algorithm working in more than two dimensions. To do this, we can run our perceptron algorithm on data with 5 features.\nFirst we need to generate perceptron data with 5 features. To do this we need to modify the perceptron_data function that we have been using. We’ll also make the noise 0.35 (which is just the average of the noises from the pervious experiments)\n\ndef multi_dim_perceptron_data(n_points = 300, noise = 0.35, dim=5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, dim))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = multi_dim_perceptron_data(dim=5)\n\nWe can look at the score of our perceptron over the training period to help us answer if the data is linearly separable. First we need to run our algorithm.\n\n# rerun perceptron for experiment 3\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\nloss = 1\nloss_vec = []\nscores = []\nwhile loss &gt; 0:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(xi, yi)\n    scores.append( p.score(X))\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nNow to check if the data is linearly separable, we need to check if all the dot products between the weight vector w and feature vector xi are the same signs. This is because if the score and the target vector have opposite signs then the perceptron needs to update and we have not found a perfectly correct answer.\n\nclassification = p.score(X)*y\nclassification.min()\n\ntensor(0.3203)\n\n\nSince our minimum value in classification is 0.0`68 we know that for any value of i, &lt;w,xi&gt; &gt; 0 if y = 1, and thus we have a linearly separable dataset.\nTo be extra sure we can also look at the loss and see if the perceptron converges such that it comes to the right answer and has a loss of 0.\n\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 3')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"final loss: \", loss_vec[len(loss_vec)-1])\n\n\n\n\n\n\n\n\nfinal loss:  0.0\n\n\nWe see that our loss does indeed eventually go to 0 further indicating our perceptron settled on a hyperplane that perfectly separates the data.\nTherefore, the dataset in this experiment is linearly separable"
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#part-c-minibatch-perceptron",
    "href": "posts/implementing-perceptron/index.html#part-c-minibatch-perceptron",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Now we can look at how a minibatch perceptron performs. Instead of looking at a single point during each iteration, we will look at k randomly selected points. In addition to this change we add a learning rate alpha that changes how much the weights change with each iteration.\nTo do this we should update our grad function. The code for the grad function and the whole minibatch perceptron function is here.\nWith our implmentation of the minibatch perceptron we can canduct a few experiments.\n\n\nWe should first check to see that our minibatch perceptron performs similarly to a regular perceptron when it is only looking at one point (just like a regular perceptron). In this case, k = 1.\nWe will first look at how a minibatch perceptron performs compared to a regular perceptron when the data is linearly separable. To do this we will see if the minibatch perceptron converges to a loss of 0 and look at its loss graph.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nperceptron_data(n_points = 300, noise = 0.2)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n\nloss_graph(loss_vec=loss_vec)\n\ntensor(0.)\nfinal loss:  tensor(0.)\n\n\n\n\n\n\n\n\n\nIt takes more steps, but we do find that the perceptron converges to a loss of 0.\nNow let’s look at how the minibatch perceptron performs with overlapping data.\n\nX, y = perceptron_data(noise=0.5)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\niterations = 0\nwhile iterations &lt; 1000: \n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n        \n    iterations += 1\n\nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.08666666597127914\n\n\nAfter 1000 iterations we see that just like the regular perceptron, the minibatch perceptron struggles a little bit but still finds a hyperplane with a low loss."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#experiment-2-1",
    "href": "posts/implementing-perceptron/index.html#experiment-2-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Now looking at 5 dimensional data where k=10, we should also find that the minibatch perceptron finds a separating line.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nX, y = multi_dim_perceptron_data(n_points = 300, noise = 0.2, dim=5)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n\nloss_graph(loss_vec=loss_vec)\n\ntensor(0.)\nfinal loss:  tensor(0.)\n\n\n\n\n\n\n\n\n\nAs expected, the minibatch perceptron does indeed find a separating line.\n\n\nNow what happens if k = n, that is we check the entire data set at once. Even if the data is not linearly separable, our minibatch perceptron should be able to converge.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nX, y = perceptron_data(n_points = 300, noise = 0.5)\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\niterations = 0\nwhile iterations &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = n\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n        \n    iterations += 1\n\nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.07666666805744171\n\n\nWe see that our loss converges to around 0.07, which is pretty small, even though the data is not linearly separable."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#part-d",
    "href": "posts/implementing-perceptron/index.html#part-d",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The runtime complexity of a single iteration is O(N) where N is the number of features (the size of a single row in the feature matrix) This is because the number of computations that occur in a single iteration depends on the dot product between w and Xi where X is the feature matrix and i is a single row in the matrix.\nThe minibatch is essentially the same but instead of looking at just one row of the feature matrix, you are looking at k rows in a single iteration. So the runtime becomes O(KN) where K is the number of rows you are looking at in a single iteration.\nPerceptron grad works by computing the score for a single data point (the dot product between the weight and all the features of the data point). If the score multiplied by the target is below 0 then we have misclassified and our weight is not perfectly accurate. If this is true then we want to multiply Xi * yi to get how much we should change the weight by for each feature (we end up adding this value to our weight in our step function). If we don’t misclassify then we are fine and we return a vector of 0s so we don’t change our weight by anything in the step function."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#conclusion",
    "href": "posts/implementing-perceptron/index.html#conclusion",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The perceptron algorithm works by continuously updating its weight according to on how accurately it classified the data points given to it. Implementing the perceptron and minibatch perceptron gave me a solid foundational understanding of the perceptron’s design as well as situations where it is well suited to separating data and situations where it is disadvantaged. The experiments helped me further understand the perceptron’s performance over linearly separable data, non-linearly separable data, and multi-dimensional data. Further experimentation with the minibatch perceptron helped me understand how it can converge when a regular perceptron can’t like when it looks at all the data points in a single iteration."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Mihir Singh\n\n\nBy developing a model to classify the Palmer penguins, there can be a more accurate species identification of penguins given a certain set of features. Comparing the different features in the Palmer penguin data set, it was determined that the culmen length, culmen depth, and island are useful features for determining a penguin’s species. Different types of classification models were also compared (logistic regress, decision tree, and SVC). It was determined that a logistic regression model looking at a penguins island of residence, culmen depth, and culmen depth would be a highly accurate model. When it was applied to the testing data, it received an accuracy score of %100.\nFirst we must access the training data. We can also import any libraries we might need later on.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nThis code prepares qualitative columns in the data. Provided by Professor Phil Chodrow.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNo we can then create visualizations that give us some new understanding about the Palmer Penguins.\nWe can first look to see if there is any difference in body mass by island. To see this, we can create a box plot with the body mass being on the y-axis and the islands being the discrete categories making up the x-axis.\n\np1 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", )\n\n\n\n\n\n\n\n\nIn p1 we can see that penguins on Biscoe island are generally much larger in body mass than penguins on other islands.\nWe can then also look to see if the difference is holds within species. That is, is a Chinstrap penguin on Biscoe island generally bigger than a Chinstrap penguin on Torgersen island, or is it that a certain species of penguin dominates Biscoe island.\n\np2 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = \"Species\" ,palette=[\"r\", \"g\", 'b'])\n\n\n\n\n\n\n\n\nIn p2 we find that a reason that the penguins on Biscoe Island are heavier may be because of its population of Gentoo Penguins. There are no other islands with a Gentoo penguin population, and they seem to be much heavier than other penguin species.\nWe can check this by looking at the mean body mass of penguins across species.\n\nsummaryTable = train.groupby(['Species']).aggregate(avgMass=('Body Mass (g)', 'mean'))\nprint(summaryTable)\n\n                                               avgMass\nSpecies                                               \nAdelie Penguin (Pygoscelis adeliae)        3718.487395\nChinstrap penguin (Pygoscelis antarctica)  3743.421053\nGentoo penguin (Pygoscelis papua)          5039.948454\n\n\nIndeed, we see that Gentoo penguins are, on average, much heavier than other penguins. So it makes sense to conclude that their prescence on Biscoe Island and abscence on other islands might be a big reason the average penguin on Biscoe Island is larger than the average penguin on other islands.\nIt might also be interesting to see if penguins with a bigger culmen length or depth are heavier. Again we can plot body mass and culmen length/depth to see this relationship.\n\np3 = sns.boxplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\np3 = sns.boxplot(train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\")\np3.set_xticks([0,10,20,30,40,50,60,70])\n\n\n\n\n\n\n\n\nWe see that as culmen length increases, the mass of the penguin generally increases, but the same is not true for culmen depth. In fact, the body mass is highest when the culmen depth reaches around 15-16 mm. These measurements line up with the culmen depth range of Chinstrap penguins so we can conclude that culmen depth doesn’t really have that much of an effect on body mass.\n\n\n\nFrom our visualizations we can infer that body mass and the island the penguin is on might be useful features. However, we should probably look for 3 good features.\nWe can start by eliminating features with low variance.\n\nfrom sklearn.feature_selection import VarianceThreshold\nX = X_train\n\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nnewTraining = sel.fit_transform(X)\n\nFirst, we can iterate through every qualitative and quantitative variable and create different combinations of features to test.\nThen we can start preparing models. The models I will use are a logistic regression, decision tree, and svc classifier.\nFinally, we can cross-validate these models.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import cross_val_score\n\n# values to keep track of best score from each classifier\nLRBest = 0\nDTBest = 0\nSVCBest = 0\nLRBestCols = []\nDTBestCols = []\nSVCBestCols = []\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    \n    # logistic regression\n    LR = LogisticRegression(max_iter=5000)\n    LR.fit(X_train[cols], y_train)\n    scoreLR = LR.score(X_train[cols], y_train)\n    if scoreLR &gt; LRBest:\n      LRBest = scoreLR\n      LRBestCols = cols\n    \n    # decision tree\n    DT = DecisionTreeClassifier(max_depth=20)\n    DT.fit(X_train[cols], y_train)\n    scoreDT = DT.score(X_train[cols], y_train)\n    if scoreDT &gt; DTBest:\n      DTBest = scoreDT\n      DTBestCols = cols\n    \n    # SVC\n    SVCModel = SVC(gamma='auto')\n    SVCModel.fit(X_train[cols], y_train)\n    scoreSVC = SVCModel.score(X_train[cols], y_train)\n    if scoreSVC &gt; SVCBest:\n      SVCBest = scoreSVC\n      SVCBestCols = cols\n\nprint(LRBest, \" \", LRBestCols)\nprint(DTBest, \" \", DTBestCols)\nprint(SVCBest, \" \", SVCBestCols)\n\n# cross validate models\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\nprint((cv_scores_LR).mean())\n\ncv_scores_DT = cross_val_score(DT, X_train, y_train, cv=5)\nprint((cv_scores_DT).mean())\n\ncv_scores_SVC = cross_val_score(SVCModel, X_train, y_train, cv=5)\nprint((cv_scores_SVC).mean())\n\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0   ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n0.9569381598793363\n0.6407239819004525\n\n\nWhile we do find that the decision tree classifier does have a After cross-validating the models, it seems that a logistic regression is best. We can now look at creating a logistic regression model with the three best features for it (which were Island, Culmen Length, and Culmen Depth).\nWe found these three best features by running through every possible combination of three features (with one qualitative and 2 quantitative) and keep track of which combination led to the high model score.\n\n\n\nWith our models and features selected, we can test our decisions/model.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\n\n\nWe can plot the decision regions of our classifiers to get a better understanding of the conclusions our model drew.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at a confusion matrix to give us a better understanding of our model’s accuracy.\n\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = LR.predict(X_test[cols])\ny_test_pred\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nFrom this we see that we correctly predict every penguin. Since there are no values outside the diagonal of the matrix, we don’t have any false predictions. Given that we use island as a feature, we will probably see more errors predicting Adelie penguins as Chinstrap or Gentoo. This is because the Adelie penguin is the only penguin species that is present on every island so it is probably easiest for our model to confuse it for another species.\n\n\n\nA logistic regression model trained by looking at the Palmer penguin dataset features of culmen length, culmen depth, and home island was highly accurate, getting an accuracy score of %100 on the test data.\nBy finding this model, I learned about the importance of conducting thorough research on your data. Much of the initial work did not involve creating models but exploring the features and characteristics of the dataset. Only after this exploration was I able to develop an intuition for what features might be useful in creating a model. However, just exploring the data is not enough to create a good model, cross-validation and eliminating features with low variance were also important techniques employed in this process, guiding against overfitting and helping narrow down on useful features respectively."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#abstract",
    "href": "posts/classifying-palmer-penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "By developing a model to classify the Palmer penguins, there can be a more accurate species identification of penguins given a certain set of features. Comparing the different features in the Palmer penguin data set, it was determined that the culmen length, culmen depth, and island are useful features for determining a penguin’s species. Different types of classification models were also compared (logistic regress, decision tree, and SVC). It was determined that a logistic regression model looking at a penguins island of residence, culmen depth, and culmen depth would be a highly accurate model. When it was applied to the testing data, it received an accuracy score of %100.\nFirst we must access the training data. We can also import any libraries we might need later on.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#data-preparation",
    "href": "posts/classifying-palmer-penguins/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This code prepares qualitative columns in the data. Provided by Professor Phil Chodrow.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#explore",
    "href": "posts/classifying-palmer-penguins/index.html#explore",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "No we can then create visualizations that give us some new understanding about the Palmer Penguins.\nWe can first look to see if there is any difference in body mass by island. To see this, we can create a box plot with the body mass being on the y-axis and the islands being the discrete categories making up the x-axis.\n\np1 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", )\n\n\n\n\n\n\n\n\nIn p1 we can see that penguins on Biscoe island are generally much larger in body mass than penguins on other islands.\nWe can then also look to see if the difference is holds within species. That is, is a Chinstrap penguin on Biscoe island generally bigger than a Chinstrap penguin on Torgersen island, or is it that a certain species of penguin dominates Biscoe island.\n\np2 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = \"Species\" ,palette=[\"r\", \"g\", 'b'])\n\n\n\n\n\n\n\n\nIn p2 we find that a reason that the penguins on Biscoe Island are heavier may be because of its population of Gentoo Penguins. There are no other islands with a Gentoo penguin population, and they seem to be much heavier than other penguin species.\nWe can check this by looking at the mean body mass of penguins across species.\n\nsummaryTable = train.groupby(['Species']).aggregate(avgMass=('Body Mass (g)', 'mean'))\nprint(summaryTable)\n\n                                               avgMass\nSpecies                                               \nAdelie Penguin (Pygoscelis adeliae)        3718.487395\nChinstrap penguin (Pygoscelis antarctica)  3743.421053\nGentoo penguin (Pygoscelis papua)          5039.948454\n\n\nIndeed, we see that Gentoo penguins are, on average, much heavier than other penguins. So it makes sense to conclude that their prescence on Biscoe Island and abscence on other islands might be a big reason the average penguin on Biscoe Island is larger than the average penguin on other islands.\nIt might also be interesting to see if penguins with a bigger culmen length or depth are heavier. Again we can plot body mass and culmen length/depth to see this relationship.\n\np3 = sns.boxplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\np3 = sns.boxplot(train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\")\np3.set_xticks([0,10,20,30,40,50,60,70])\n\n\n\n\n\n\n\n\nWe see that as culmen length increases, the mass of the penguin generally increases, but the same is not true for culmen depth. In fact, the body mass is highest when the culmen depth reaches around 15-16 mm. These measurements line up with the culmen depth range of Chinstrap penguins so we can conclude that culmen depth doesn’t really have that much of an effect on body mass."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#choosing-features-and-models",
    "href": "posts/classifying-palmer-penguins/index.html#choosing-features-and-models",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "From our visualizations we can infer that body mass and the island the penguin is on might be useful features. However, we should probably look for 3 good features.\nWe can start by eliminating features with low variance.\n\nfrom sklearn.feature_selection import VarianceThreshold\nX = X_train\n\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nnewTraining = sel.fit_transform(X)\n\nFirst, we can iterate through every qualitative and quantitative variable and create different combinations of features to test.\nThen we can start preparing models. The models I will use are a logistic regression, decision tree, and svc classifier.\nFinally, we can cross-validate these models.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import cross_val_score\n\n# values to keep track of best score from each classifier\nLRBest = 0\nDTBest = 0\nSVCBest = 0\nLRBestCols = []\nDTBestCols = []\nSVCBestCols = []\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    \n    # logistic regression\n    LR = LogisticRegression(max_iter=5000)\n    LR.fit(X_train[cols], y_train)\n    scoreLR = LR.score(X_train[cols], y_train)\n    if scoreLR &gt; LRBest:\n      LRBest = scoreLR\n      LRBestCols = cols\n    \n    # decision tree\n    DT = DecisionTreeClassifier(max_depth=20)\n    DT.fit(X_train[cols], y_train)\n    scoreDT = DT.score(X_train[cols], y_train)\n    if scoreDT &gt; DTBest:\n      DTBest = scoreDT\n      DTBestCols = cols\n    \n    # SVC\n    SVCModel = SVC(gamma='auto')\n    SVCModel.fit(X_train[cols], y_train)\n    scoreSVC = SVCModel.score(X_train[cols], y_train)\n    if scoreSVC &gt; SVCBest:\n      SVCBest = scoreSVC\n      SVCBestCols = cols\n\nprint(LRBest, \" \", LRBestCols)\nprint(DTBest, \" \", DTBestCols)\nprint(SVCBest, \" \", SVCBestCols)\n\n# cross validate models\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\nprint((cv_scores_LR).mean())\n\ncv_scores_DT = cross_val_score(DT, X_train, y_train, cv=5)\nprint((cv_scores_DT).mean())\n\ncv_scores_SVC = cross_val_score(SVCModel, X_train, y_train, cv=5)\nprint((cv_scores_SVC).mean())\n\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0   ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n0.9569381598793363\n0.6407239819004525\n\n\nWhile we do find that the decision tree classifier does have a After cross-validating the models, it seems that a logistic regression is best. We can now look at creating a logistic regression model with the three best features for it (which were Island, Culmen Length, and Culmen Depth).\nWe found these three best features by running through every possible combination of three features (with one qualitative and 2 quantitative) and keep track of which combination led to the high model score."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#testing",
    "href": "posts/classifying-palmer-penguins/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "With our models and features selected, we can test our decisions/model.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test)\n\n1.0"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "We can plot the decision regions of our classifiers to get a better understanding of the conclusions our model drew.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \nplot_regions(LR, X_train[cols], y_train)"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#confusion-matrix",
    "href": "posts/classifying-palmer-penguins/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "We can also take a look at a confusion matrix to give us a better understanding of our model’s accuracy.\n\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = LR.predict(X_test[cols])\ny_test_pred\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nFrom this we see that we correctly predict every penguin. Since there are no values outside the diagonal of the matrix, we don’t have any false predictions. Given that we use island as a feature, we will probably see more errors predicting Adelie penguins as Chinstrap or Gentoo. This is because the Adelie penguin is the only penguin species that is present on every island so it is probably easiest for our model to confuse it for another species."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#discussion",
    "href": "posts/classifying-palmer-penguins/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "A logistic regression model trained by looking at the Palmer penguin dataset features of culmen length, culmen depth, and home island was highly accurate, getting an accuracy score of %100 on the test data.\nBy finding this model, I learned about the importance of conducting thorough research on your data. Much of the initial work did not involve creating models but exploring the features and characteristics of the dataset. Only after this exploration was I able to develop an intuition for what features might be useful in creating a model. However, just exploring the data is not enough to create a good model, cross-validation and eliminating features with low variance were also important techniques employed in this process, guiding against overfitting and helping narrow down on useful features respectively."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]
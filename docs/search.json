[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Mihir Singh"
  },
  {
    "objectID": "posts/implementing-perceptron/index.html",
    "href": "posts/implementing-perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Mihir Singh\n\n\nThis blog posts first implements a perceptron algorithm and a minibatch perceptron algorithm. For the perceptron algorithm, experiments were run to check how it performed on linearly separable data, non-linearly separable data, and data with more than 2 dimensions. Experiments with the minibatch perceptron algorithm first focused on its performance compared to the regular perceptron algorithm, its performance on data with multiple dimensions, and its ability to converge when looking at all the data in a single iteration. Through this process, I developed an understanding of the perceptron algorithm and how it can be improved upon with a minibatch algorithm.\n\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\nThe code for the perceptron is here.\nFor our experiments we will use data from the in-class perceptron warmup which we know is linearly separable.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nFrom this we get a visual confirmation that our data is linearly separable. We should first check if our perceptron probably works by running it on our minimal training loop. We know it works if our perceptron converges and we achieve a loss of 0.\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # look at a single data point - consulted with Sophie Seiple to get single point  \n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n    \n\ntensor(0.)\n\n\nWe see that our tensor eventually becomes 0, so we are indeed converging on 0 and our perceptron is probably correct.\n\n\n\nNow we can experiment with our perceptron and look at when the data is linearly separable, not linearly separable, and look at how the perceptron works in 2 dimensions.\n\n\nWe can use the same data from the example above (since we know that the data is linearly separable) and visualize the process of a perceptron finding a line the separates the data.\n\n# Code below provided by Professor Phil Chodrow\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# functions for plotting perceptron data\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nJust to be sure, let’s look at the data first to confirm it looks linearly separable visually.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X,y, ax)\n\n\n\n\n\n\n\n\nIndeed, our data looks linearly separable. Now let’s run our perceptron.\n\n# Parts of code for visualization provided by Professor Phil Chodrow\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n\nwhile loss &gt; 0:\n    # we are always still look at a single data point\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    \n    ax = axarr.ravel()[current_ax]\n    \n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value     \n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n        \nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe can see that our perceptron becomes more accurate, converging on the correct answer. Another way to visualize our perceptron’s accuracy is through a loss graph.\n\n# Loss graph\ndef loss_graph(loss_vec):\n    plt.plot(loss_vec, marker='o', linestyle='-')\n    plt.title('Loss Graph: Experiment 1')\n    plt.xlabel('Step')\n    plt.ylabel('Loss')\n    plt.show()\n    print(\"final loss: \", loss_vec[len(loss_vec)-1])\n    \nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.0\n\n\nWe see that eventually (after may steps) we eventually do reach a loss of 0., but our loss graph shows the accuracy of the perceptron for every step over time.\nLet’s see what the decision boundary looks like on the final iteration.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nWe see that on the final step, the perceptron settles on a line that accurately separated all the data.\n\n\n\nWe’ve looked at the perceptron’s accuracy when we know that the data is linearly separable. Now we can look at how a perceptron will perform when we know that the data is not linearly separable.\nTo do this, we first need to create data that we know is not linearly separable. We can use the same perceptron_data function from before, but we can instead set it so that the noise is higher. This has the effect of points from the different classes crossing each other’s boundaries and thus making the data not linearly separable.\n\nX, y = perceptron_data(noise=0.5)\ny_ = (y+1)/2\n\nLet’s also plot the training and testing data for a visual inspection.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X,y, ax)\n\n\n\n\n\n\n\n\nNow we can visually see that out data is not perfectly linearly separable.\nLet’s begin training our perceptron. We will loop through for 1000 iterations. Let’s see what we come up with after 500 and then finally when we finish at 1000 iterations\n\n# Initialize perceptron\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\n# Training loop to 500\niterations = 0\nwhile iterations &lt; 500:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    iterations += 1\n    loss_vec.append(loss)\n    \n# Plot perceptron after 500 iterations\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nprint(loss)\n\n0.1666666716337204\n\n\n\n\n\n\n\n\n\nIt is somewhat accurate, but it’s a stretch to say perfect like in experiment 1. Let’s go to 1000 iterations. For the final iteration, we will also plot the decision boundary of the perceptron before we stopped iterating.\n\n# Training loop to 1000\niterations = 0\nloss_vec = []\n\nwhile iterations &lt; 1000:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    iterations += 1\n    loss_vec.append(loss)\n    \n# Plot perceptron after 500 iterations (with decision boundaries)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nprint(\"loss:\", loss)\n\nloss: 0.1066666692495346\n\n\n\n\n\n\n\n\n\nWe see here that our loss decreases slightly. Let’s take a look at the loss over time.\n\nprint(len(loss_vec))\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 2')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"final loss: \", loss_vec[len(loss_vec)-1])\n\n1000\nfinal loss:  0.1066666692495346\n\n\n\n\n\n\n\n\n\nWe can see that we can’t say the loss generally decreases and converges on an answer - like we could generalize in part 1.\n\n\n\nNow we will look at the perceptron algorithm working in more than two dimensions. To do this, we can run our perceptron algorithm on data with 5 features.\nFirst we need to generate perceptron data with 5 features. To do this we need to modify the perceptron_data function that we have been using. We’ll also make the noise 0.35 (which is just the average of the noises from the pervious experiments)\n\ndef multi_dim_perceptron_data(n_points = 300, noise = 0.35, dim=5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, dim))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = multi_dim_perceptron_data(dim=5)\n\nWe can look at the score of our perceptron over the training period to help us answer if the data is linearly separable. First we need to run our algorithm.\n\n# rerun perceptron for experiment 3\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\nloss = 1\nloss_vec = []\nscores = []\nwhile loss &gt; 0:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(xi, yi)\n    scores.append( p.score(X))\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nNow to check if the data is linearly separable, we need to check if all the dot products between the weight vector w and feature vector xi are the same signs. This is because if the score and the target vector have opposite signs then the perceptron needs to update and we have not found a perfectly correct answer.\n\nclassification = p.score(X)*y\nclassification.min()\n\ntensor(0.3203)\n\n\nSince our minimum value in classification is 0.0`68 we know that for any value of i, &lt;w,xi&gt; &gt; 0 if y = 1, and thus we have a linearly separable dataset.\nTo be extra sure we can also look at the loss and see if the perceptron converges such that it comes to the right answer and has a loss of 0.\n\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 3')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"final loss: \", loss_vec[len(loss_vec)-1])\n\n\n\n\n\n\n\n\nfinal loss:  0.0\n\n\nWe see that our loss does indeed eventually go to 0 further indicating our perceptron settled on a hyperplane that perfectly separates the data.\nTherefore, the dataset in this experiment is linearly separable\n\n\n\n\nNow we can look at how a minibatch perceptron performs. Instead of looking at a single point during each iteration, we will look at k randomly selected points. In addition to this change we add a learning rate alpha that changes how much the weights change with each iteration.\nTo do this we should update our grad function. The code for the grad function and the whole minibatch perceptron function is here.\nWith our implmentation of the minibatch perceptron we can canduct a few experiments.\n\n\nWe should first check to see that our minibatch perceptron performs similarly to a regular perceptron when it is only looking at one point (just like a regular perceptron). In this case, k = 1.\nWe will first look at how a minibatch perceptron performs compared to a regular perceptron when the data is linearly separable. To do this we will see if the minibatch perceptron converges to a loss of 0 and look at its loss graph.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nperceptron_data(n_points = 300, noise = 0.2)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n\nloss_graph(loss_vec=loss_vec)\n\ntensor(0.)\nfinal loss:  tensor(0.)\n\n\n\n\n\n\n\n\n\nIt takes more steps, but we do find that the perceptron converges to a loss of 0.\nNow let’s look at how the minibatch perceptron performs with overlapping data.\n\nX, y = perceptron_data(noise=0.5)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\niterations = 0\nwhile iterations &lt; 1000: \n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n        \n    iterations += 1\n\nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.08666666597127914\n\n\nAfter 1000 iterations we see that just like the regular perceptron, the minibatch perceptron struggles a little bit but still finds a hyperplane with a low loss.\n\n\n\n\nNow looking at 5 dimensional data where k=10, we should also find that the minibatch perceptron finds a separating line.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nX, y = multi_dim_perceptron_data(n_points = 300, noise = 0.2, dim=5)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n\nloss_graph(loss_vec=loss_vec)\n\ntensor(0.)\nfinal loss:  tensor(0.)\n\n\n\n\n\n\n\n\n\nAs expected, the minibatch perceptron does indeed find a separating line.\n\n\nNow what happens if k = n, that is we check the entire data set at once. Even if the data is not linearly separable, our minibatch perceptron should be able to converge.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nX, y = perceptron_data(n_points = 300, noise = 0.5)\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\niterations = 0\nwhile iterations &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = n\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n        \n    iterations += 1\n\nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.07666666805744171\n\n\nWe see that our loss converges to around 0.07, which is pretty small, even though the data is not linearly separable.\n\n\n\n\nThe runtime complexity of a single iteration is O(N) where N is the number of features (the size of a single row in the feature matrix) This is because the number of computations that occur in a single iteration depends on the dot product between w and Xi where X is the feature matrix and i is a single row in the matrix.\nThe minibatch is essentially the same but instead of looking at just one row of the feature matrix, you are looking at k rows in a single iteration. So the runtime becomes O(KN) where K is the number of rows you are looking at in a single iteration.\nPerceptron grad works by computing the score for a single data point (the dot product between the weight and all the features of the data point). If the score multiplied by the target is below 0 then we have misclassified and our weight is not perfectly accurate. If this is true then we want to multiply Xi * yi to get how much we should change the weight by for each feature (we end up adding this value to our weight in our step function). If we don’t misclassify then we are fine and we return a vector of 0s so we don’t change our weight by anything in the step function.\n\n\n\nThe perceptron algorithm works by continuously updating its weight according to on how accurately it classified the data points given to it. Implementing the perceptron and minibatch perceptron gave me a solid foundational understanding of the perceptron’s design as well as situations where it is well suited to separating data and situations where it is disadvantaged. The experiments helped me further understand the perceptron’s performance over linearly separable data, non-linearly separable data, and multi-dimensional data. Further experimentation with the minibatch perceptron helped me understand how it can converge when a regular perceptron can’t like when it looks at all the data points in a single iteration."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#abstract",
    "href": "posts/implementing-perceptron/index.html#abstract",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "This blog posts first implements a perceptron algorithm and a minibatch perceptron algorithm. For the perceptron algorithm, experiments were run to check how it performed on linearly separable data, non-linearly separable data, and data with more than 2 dimensions. Experiments with the minibatch perceptron algorithm first focused on its performance compared to the regular perceptron algorithm, its performance on data with multiple dimensions, and its ability to converge when looking at all the data in a single iteration. Through this process, I developed an understanding of the perceptron algorithm and how it can be improved upon with a minibatch algorithm.\n\n\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#implementation",
    "href": "posts/implementing-perceptron/index.html#implementation",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The code for the perceptron is here.\nFor our experiments we will use data from the in-class perceptron warmup which we know is linearly separable.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,2))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nFrom this we get a visual confirmation that our data is linearly separable. We should first check if our perceptron probably works by running it on our minimal training loop. We know it works if our perceptron converges and we achieve a loss of 0.\n\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # look at a single data point - consulted with Sophie Seiple to get single point  \n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n    \n\ntensor(0.)\n\n\nWe see that our tensor eventually becomes 0, so we are indeed converging on 0 and our perceptron is probably correct."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#part-b-experiments",
    "href": "posts/implementing-perceptron/index.html#part-b-experiments",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Now we can experiment with our perceptron and look at when the data is linearly separable, not linearly separable, and look at how the perceptron works in 2 dimensions.\n\n\nWe can use the same data from the example above (since we know that the data is linearly separable) and visualize the process of a perceptron finding a line the separates the data.\n\n# Code below provided by Professor Phil Chodrow\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# functions for plotting perceptron data\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nJust to be sure, let’s look at the data first to confirm it looks linearly separable visually.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X,y, ax)\n\n\n\n\n\n\n\n\nIndeed, our data looks linearly separable. Now let’s run our perceptron.\n\n# Parts of code for visualization provided by Professor Phil Chodrow\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n\nwhile loss &gt; 0:\n    # we are always still look at a single data point\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    \n    ax = axarr.ravel()[current_ax]\n    \n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n    \n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value     \n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n        \nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe can see that our perceptron becomes more accurate, converging on the correct answer. Another way to visualize our perceptron’s accuracy is through a loss graph.\n\n# Loss graph\ndef loss_graph(loss_vec):\n    plt.plot(loss_vec, marker='o', linestyle='-')\n    plt.title('Loss Graph: Experiment 1')\n    plt.xlabel('Step')\n    plt.ylabel('Loss')\n    plt.show()\n    print(\"final loss: \", loss_vec[len(loss_vec)-1])\n    \nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.0\n\n\nWe see that eventually (after may steps) we eventually do reach a loss of 0., but our loss graph shows the accuracy of the perceptron for every step over time.\nLet’s see what the decision boundary looks like on the final iteration.\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nWe see that on the final step, the perceptron settles on a line that accurately separated all the data.\n\n\n\nWe’ve looked at the perceptron’s accuracy when we know that the data is linearly separable. Now we can look at how a perceptron will perform when we know that the data is not linearly separable.\nTo do this, we first need to create data that we know is not linearly separable. We can use the same perceptron_data function from before, but we can instead set it so that the noise is higher. This has the effect of points from the different classes crossing each other’s boundaries and thus making the data not linearly separable.\n\nX, y = perceptron_data(noise=0.5)\ny_ = (y+1)/2\n\nLet’s also plot the training and testing data for a visual inspection.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X,y, ax)\n\n\n\n\n\n\n\n\nNow we can visually see that out data is not perfectly linearly separable.\nLet’s begin training our perceptron. We will loop through for 1000 iterations. Let’s see what we come up with after 500 and then finally when we finish at 1000 iterations\n\n# Initialize perceptron\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\nloss_vec = []\n\n# Training loop to 500\niterations = 0\nwhile iterations &lt; 500:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    iterations += 1\n    loss_vec.append(loss)\n    \n# Plot perceptron after 500 iterations\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nprint(loss)\n\n0.1666666716337204\n\n\n\n\n\n\n\n\n\nIt is somewhat accurate, but it’s a stretch to say perfect like in experiment 1. Let’s go to 1000 iterations. For the final iteration, we will also plot the decision boundary of the perceptron before we stopped iterating.\n\n# Training loop to 1000\niterations = 0\nloss_vec = []\n\nwhile iterations &lt; 1000:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    local_loss = opt.step(xi, yi)\n    loss = p.loss(X, y).item()\n    iterations += 1\n    loss_vec.append(loss)\n    \n# Plot perceptron after 500 iterations (with decision boundaries)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nprint(\"loss:\", loss)\n\nloss: 0.1066666692495346\n\n\n\n\n\n\n\n\n\nWe see here that our loss decreases slightly. Let’s take a look at the loss over time.\n\nprint(len(loss_vec))\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 2')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"final loss: \", loss_vec[len(loss_vec)-1])\n\n1000\nfinal loss:  0.1066666692495346\n\n\n\n\n\n\n\n\n\nWe can see that we can’t say the loss generally decreases and converges on an answer - like we could generalize in part 1.\n\n\n\nNow we will look at the perceptron algorithm working in more than two dimensions. To do this, we can run our perceptron algorithm on data with 5 features.\nFirst we need to generate perceptron data with 5 features. To do this we need to modify the perceptron_data function that we have been using. We’ll also make the noise 0.35 (which is just the average of the noises from the pervious experiments)\n\ndef multi_dim_perceptron_data(n_points = 300, noise = 0.35, dim=5):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points, dim))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = multi_dim_perceptron_data(dim=5)\n\nWe can look at the score of our perceptron over the training period to help us answer if the data is linearly separable. First we need to run our algorithm.\n\n# rerun perceptron for experiment 3\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\nloss = 1\nloss_vec = []\nscores = []\nwhile loss &gt; 0:\n    i = torch.randint(n, size = (1,))\n    xi = X[[i],:]\n    yi = y[i]\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(xi, yi)\n    scores.append( p.score(X))\n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nNow to check if the data is linearly separable, we need to check if all the dot products between the weight vector w and feature vector xi are the same signs. This is because if the score and the target vector have opposite signs then the perceptron needs to update and we have not found a perfectly correct answer.\n\nclassification = p.score(X)*y\nclassification.min()\n\ntensor(0.3203)\n\n\nSince our minimum value in classification is 0.0`68 we know that for any value of i, &lt;w,xi&gt; &gt; 0 if y = 1, and thus we have a linearly separable dataset.\nTo be extra sure we can also look at the loss and see if the perceptron converges such that it comes to the right answer and has a loss of 0.\n\nplt.plot(loss_vec, marker='o', linestyle='-')\nplt.title('Loss Graph: Experiment 3')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\nprint(\"final loss: \", loss_vec[len(loss_vec)-1])\n\n\n\n\n\n\n\n\nfinal loss:  0.0\n\n\nWe see that our loss does indeed eventually go to 0 further indicating our perceptron settled on a hyperplane that perfectly separates the data.\nTherefore, the dataset in this experiment is linearly separable"
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#part-c-minibatch-perceptron",
    "href": "posts/implementing-perceptron/index.html#part-c-minibatch-perceptron",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Now we can look at how a minibatch perceptron performs. Instead of looking at a single point during each iteration, we will look at k randomly selected points. In addition to this change we add a learning rate alpha that changes how much the weights change with each iteration.\nTo do this we should update our grad function. The code for the grad function and the whole minibatch perceptron function is here.\nWith our implmentation of the minibatch perceptron we can canduct a few experiments.\n\n\nWe should first check to see that our minibatch perceptron performs similarly to a regular perceptron when it is only looking at one point (just like a regular perceptron). In this case, k = 1.\nWe will first look at how a minibatch perceptron performs compared to a regular perceptron when the data is linearly separable. To do this we will see if the minibatch perceptron converges to a loss of 0 and look at its loss graph.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nperceptron_data(n_points = 300, noise = 0.2)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n\nloss_graph(loss_vec=loss_vec)\n\ntensor(0.)\nfinal loss:  tensor(0.)\n\n\n\n\n\n\n\n\n\nIt takes more steps, but we do find that the perceptron converges to a loss of 0.\nNow let’s look at how the minibatch perceptron performs with overlapping data.\n\nX, y = perceptron_data(noise=0.5)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\niterations = 0\nwhile iterations &lt; 1000: \n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n        \n    iterations += 1\n\nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.08666666597127914\n\n\nAfter 1000 iterations we see that just like the regular perceptron, the minibatch perceptron struggles a little bit but still finds a hyperplane with a low loss."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#experiment-2-1",
    "href": "posts/implementing-perceptron/index.html#experiment-2-1",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Now looking at 5 dimensional data where k=10, we should also find that the minibatch perceptron finds a separating line.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nX, y = multi_dim_perceptron_data(n_points = 300, noise = 0.2, dim=5)\n\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y) \n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n    \n    if(loss == 0):\n        print(loss)\n\nloss_graph(loss_vec=loss_vec)\n\ntensor(0.)\nfinal loss:  tensor(0.)\n\n\n\n\n\n\n\n\n\nAs expected, the minibatch perceptron does indeed find a separating line.\n\n\nNow what happens if k = n, that is we check the entire data set at once. Even if the data is not linearly separable, our minibatch perceptron should be able to converge.\n\nfrom perceptronMB import PerceptronMB, PerceptronOptimizerMB\nX, y = perceptron_data(n_points = 300, noise = 0.5)\n# instantiate a model and an optimizer\np = PerceptronMB() \nopt = PerceptronOptimizerMB(p)\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\niterations = 0\nwhile iterations &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    # not part of the update: just for tracking our progress  \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)  \n    \n    # get k points - code provided by Professor Phil Chodrow\n    k = n\n    ix = torch.randperm(X.size(0))[:k]\n    xi = X[ix,:]\n    yi = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(xi, yi)\n        \n    iterations += 1\n\nloss_graph(loss_vec=loss_vec)\n\n\n\n\n\n\n\n\nfinal loss:  0.07666666805744171\n\n\nWe see that our loss converges to around 0.07, which is pretty small, even though the data is not linearly separable."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#part-d",
    "href": "posts/implementing-perceptron/index.html#part-d",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The runtime complexity of a single iteration is O(N) where N is the number of features (the size of a single row in the feature matrix) This is because the number of computations that occur in a single iteration depends on the dot product between w and Xi where X is the feature matrix and i is a single row in the matrix.\nThe minibatch is essentially the same but instead of looking at just one row of the feature matrix, you are looking at k rows in a single iteration. So the runtime becomes O(KN) where K is the number of rows you are looking at in a single iteration.\nPerceptron grad works by computing the score for a single data point (the dot product between the weight and all the features of the data point). If the score multiplied by the target is below 0 then we have misclassified and our weight is not perfectly accurate. If this is true then we want to multiply Xi * yi to get how much we should change the weight by for each feature (we end up adding this value to our weight in our step function). If we don’t misclassify then we are fine and we return a vector of 0s so we don’t change our weight by anything in the step function."
  },
  {
    "objectID": "posts/implementing-perceptron/index.html#conclusion",
    "href": "posts/implementing-perceptron/index.html#conclusion",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The perceptron algorithm works by continuously updating its weight according to on how accurately it classified the data points given to it. Implementing the perceptron and minibatch perceptron gave me a solid foundational understanding of the perceptron’s design as well as situations where it is well suited to separating data and situations where it is disadvantaged. The experiments helped me further understand the perceptron’s performance over linearly separable data, non-linearly separable data, and multi-dimensional data. Further experimentation with the minibatch perceptron helped me understand how it can converge when a regular perceptron can’t like when it looks at all the data points in a single iteration."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/replication-study/index.html",
    "href": "posts/replication-study/index.html",
    "title": "Replication Study: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "Mihir Singh\n\n\nThis blog post seeks to replicate the study Dissecting racial bias in an algorithm used to manage the health of populations by Obermeyer et al (2019). It does this by analyzing and recreating figures 1 and 3 of the paper, as well as further analyzing potential racial disparity between white and black patients with fewer than 5 active chronic illnesses. Like the original study, this blog post finds that the algorithm employed for the purposes of enrolling patients into an intensive care program gives black patients lower risk scores than similar white patients, thus making it so black patients must be sicker than white patients to be enrolled. This disparity likely stems from the algorithm’s reliance on healthcare spending as a indirect measurement for health, since black patients often receive less care than white patients and therefore have lower medical expenditures.\n\n\n\nIn order to replicate this study, we need to use a randomized version of the data provided by the authors of the original study. This data preserves the same patterns and trends as the study without providing the real patient data used by the authors.\n\n# Imports and data access\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nLet’s take a look at our dataframe to see what labels we will be working with.\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nWe can see that the dataframe contains useful information like race, whether or not the patient was enrolled in the care program, as well as other health data.\nNow that we have data, we can begin working on replicating the actual findings of the paper.\n\n\n\nWe will start with figure 1 of the study which showed the number of chronic illnesses versus the algorithm-predicted risk, by race. The authors decided to plot this to understand if there were racial health disparities conditional on risk score.\n\n# Plotting figure 1 \n\n# get risk scores as percentile\ndf[\"risk_percentile\"] = (df[\"risk_score_t\"].rank(pct=True) * 100).round()\n\n# get mean number of chronic illnesses and store in new dataframe\ndf['mean_num_CI'] = df.groupby(['risk_percentile', 'race'])['gagne_sum_t'].transform('mean')\n\n# plotting\nfig, axs = plt.subplots(1,1)\naxs.set_title(\"Risk Score vs Number of Chronic Illnesses\")\naxs.set_xlabel(\"Algorithm Risk Score Percentile\")\naxs.set_ylabel(\"Mean Num of Chronic Illnesses\")\nsns.scatterplot(ax = axs,data=df,x='risk_percentile',y='mean_num_CI', hue='race')\n\n\n\n\n\n\n\n\nThis plot shows us that there is indeed a racial disparity between the algorithm’s risk score and the number of chronic illnesses a patient might have. We can see that black patients often have a higher number of chronic illnesses compared to white patients in the same risk score percentile. This means that if Patient A is Black, Patient B is White, and both patients have the exact same number of chronic illnesses, Patient A will likely receive a higher risk score from the algorithm and thus be more likely to be referred to the care management program.\n\n\n\nWe can now look at reproducing figure 3 of the study which compared the mean total medical expenditure to percentile of the algorithm’s risk score as well as the mean total medical expenditure to the number of chronic illnesses a patient had. The authors decided to plot these comparisons to determine if there was any racial disparity in cost for healthcare and as a way to check if there was any disparity in the amount of care received between black and white patients.\n\n# Plotting Figure 3\n\n# get risk percentile\ndf['risk_percentile'] = (df['risk_score_t'].rank(pct=True)*100).round()\n\n# get mean expenditure\ndf[\"cost_by_RS\"] = df.groupby(['risk_percentile','race'])['cost_t'].transform('mean')\n\n# get mean cost of patient healthcare by number of chronic illness\ndf[\"cost_by_CI\"] = df.groupby(['gagne_sum_t','race'])['cost_t'].transform('mean')\n\n# plotting\nfig, axs = plt.subplots(1,2)\naxs[0].set_yscale('log')\naxs[0].set_xlabel(\"Algorithm Risk Score Percentile\")\naxs[0].set_ylabel(\"Mean Total Medical Expenditure\")\naxs[1].set_yscale('log')\naxs[1].set_xlabel(\"Number of Chronic Illnesses\")\naxs[1].set_ylabel(\" \")\nsns.scatterplot(ax=axs[0],data=df,x='risk_percentile',y='cost_by_RS', hue='race')\nsns.scatterplot(ax=axs[1],data=df,x='gagne_sum_t',y='cost_by_CI', hue='race')\n\n\n\n\n\n\n\n\nThese plots show us that as the number of chronic illnesses increase or as your risk score increases, the higher the cost of healthcare is for a patient. This is not particularly surprising since those with higher risk scores are generally less healthy and patients with more chronic illnesses will have higher care needs. However, it is important to note that these plots still show potential evidence of a racial disparity. Most patients have 5 or fewer chronic illnesses and we see that white patients tend to have higher costs than black patients if they have 5 or fewer chronic illnesses. We also see this trend reflected when comparing our risk score and medical expenditure. We find that white patients with lower risk scores tend to have higher healthcare expenditures than black patients. This phenomenon might point to potential racial bias in access to healthcare, where white patients with lower risk scores receive more treatment or care and in turn have higher medical expenditures.\n\n\n\nWe can now try and quantify the disparity we just observed in patients that have 5 or fewer active chronic conditions.\n\n\nTo do so, we need to first determine the percentage of patients with 5 or fewer chronic conditions. If this percentage is small, it might not be worth focusing on these patients.\n\n# Determining percentage of patients with 5 or fewer active chronic conditions\npercentage = len(df.loc[df['gagne_sum_t'] &lt;= 5])/ len(df) * 100\npercentage\n\n95.53952115447689\n\n\n95.5% of patients have five or fewer active chronic conditions, so it does indeed make sense to focus on these patients since any racial disparities occurring within this subgroup of patients will be affecting an overwhelming majority of the people receiving healthcare.\nBecause medical costs can vary by orders of magnitude, we will want to look at the logorithm of the cost. However, in order to do this we also need to drop any patients who did not have any medical cost at all since log(0) is undefined.\n\n# Log-transform of cost\ndf_transform = (df.loc[df['cost_t'] &gt; 0]).copy()\ndf_transform['log_cost'] = np.log(df_transform['cost_t'])\n\nIt might also make it easier for us to do some modeling by creating a one-hot encoded column for the qualitative race variable. For us 1 means the patient is black and 0 means they are white.\nThen we will separate the data into predictor variables X and target variables Y. Our predictor variables are race and the number of active chronic conditions.\n\n# one-hot encoding of race\ndf_transform['race_binary'] = (df_transform['race'] == 'black') * 1\n\n# separation of data into predictor variables and target variables\ndf_X = df_transform[['race_binary','gagne_sum_t']]\ndf_y = df_transform['log_cost']\n\n\n\n\nWith our data prepped, we can now employ a linear regression model. However, our relationship is non-linear, so we will add polynomial features to our model. If we are going to do this, we must also determine how many polynomial features we will use. We can do this with some cross validation. For our purposes we will look from degree 1 to 20.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nLR = LinearRegression()\nscores = []\nfor i in range(20):\n  X_poly = add_polynomial_features(df_X,i)\n  #LR.fit(X_poly, df_y)\n  cross_valscore = cross_val_score(LR, X_poly, df_y)\n  score = (f\"degree {i+1}\",cross_valscore.mean())\n  scores.append(score) \n\nscores.sort(key=lambda x: x[1])\nscores\n\n[('degree 19', 0.10908769701215384),\n ('degree 20', 0.10940809669337344),\n ('degree 18', 0.1193285359520339),\n ('degree 17', 0.12852193502774437),\n ('degree 16', 0.1442007114428923),\n ('degree 3', 0.14537952700038775),\n ('degree 1', 0.14538846793594346),\n ('degree 2', 0.14538846793594346),\n ('degree 5', 0.146909632661908),\n ('degree 4', 0.14699336604386143),\n ('degree 6', 0.1473340656385787),\n ('degree 7', 0.14776094174784324),\n ('degree 15', 0.148069648494553),\n ('degree 8', 0.1480739986116853),\n ('degree 9', 0.14811660549076067),\n ('degree 10', 0.14820529985520275),\n ('degree 12', 0.14824297239083922),\n ('degree 13', 0.1482567134543757),\n ('degree 14', 0.14828133604468716),\n ('degree 11', 0.14830062340213399)]\n\n\nDegree 11 seems to have the highest score, so we will use that. Now we will run our model\n\n# final model\nnew_LR = LinearRegression()\nnew_df_X = add_polynomial_features(df_X, 11)\nnew_LR.fit(new_df_X, df_y)\n\nprint(new_df_X.columns)\nnew_LR.coef_\n\nIndex(['race_binary', 'gagne_sum_t', 'poly_1', 'poly_2', 'poly_3', 'poly_4',\n       'poly_5', 'poly_6', 'poly_7', 'poly_8', 'poly_9', 'poly_10'],\n      dtype='object')\n\n\narray([-2.66958964e-01,  6.12454343e-01,  6.12451935e-01, -1.49609200e+00,\n        9.79709805e-01, -3.45158893e-01,  7.23069132e-02, -9.40470735e-03,\n        7.65069325e-04, -3.78086925e-05,  1.03681896e-06, -1.20918699e-08])\n\n\nWe see that our race coefficient is the first column of our X and therefore is the first coefficient in our linear regression. Now we calculate our wb.\n\nwb = np.exp(new_LR.coef_[0])\nwb\n\n0.7657044921324225\n\n\nAfter running our model, we find that the cost incurred by black patients is about 76.5% that of white patients when considering patients with 5 or fewer chronic conditions. This finding supports the argument of Obermeyer et al. that black patients generate lower costs because they might not receive healthcare due to racial biases.\n\n\n\n\nLike the original study, I found that there exists a racial bias in this algorithm. Black patients with the same risk score as white patients often have lower risk scores, making it harder for them to be enrolled into an intensive care program. This disparity occurs because the algorithm uses healthcare spending as a proxy for health, but black patients often have lower healthcare costs because they have less access to healthcare. Therefore, like the journal article, we find evidence of racial disparity in the algorithm.\nThe algorithm fails the test of sufficiency outlined in the formal statistical discrimination criteria discussed by Barocas, Hardt, and Narayanan (2023). Sufficiency seeks to determine if we will get a certain outcome given a certain prediction. However, the algorithm very clearly gives different outcomes dependent on race, with black patients being more sick than white patients with the same risk score. THe algorithm might predict a black patient to not be qualified for the program, but a white patient will be, while indeed both might actually need to be enrolled. The algorithm could also fail a test of independence since the proxy variable (healthcare costs) affects the score, but healthcare costs are influenced by race and thus the score could be as well."
  },
  {
    "objectID": "posts/replication-study/index.html#abstract",
    "href": "posts/replication-study/index.html#abstract",
    "title": "Replication Study: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "This blog post seeks to replicate the study Dissecting racial bias in an algorithm used to manage the health of populations by Obermeyer et al (2019). It does this by analyzing and recreating figures 1 and 3 of the paper, as well as further analyzing potential racial disparity between white and black patients with fewer than 5 active chronic illnesses. Like the original study, this blog post finds that the algorithm employed for the purposes of enrolling patients into an intensive care program gives black patients lower risk scores than similar white patients, thus making it so black patients must be sicker than white patients to be enrolled. This disparity likely stems from the algorithm’s reliance on healthcare spending as a indirect measurement for health, since black patients often receive less care than white patients and therefore have lower medical expenditures."
  },
  {
    "objectID": "posts/replication-study/index.html#part-a-data-access",
    "href": "posts/replication-study/index.html#part-a-data-access",
    "title": "Replication Study: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "In order to replicate this study, we need to use a randomized version of the data provided by the authors of the original study. This data preserves the same patterns and trends as the study without providing the real patient data used by the authors.\n\n# Imports and data access\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\nLet’s take a look at our dataframe to see what labels we will be working with.\n\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nWe can see that the dataframe contains useful information like race, whether or not the patient was enrolled in the care program, as well as other health data.\nNow that we have data, we can begin working on replicating the actual findings of the paper."
  },
  {
    "objectID": "posts/replication-study/index.html#part-b-reproducing-figure-1",
    "href": "posts/replication-study/index.html#part-b-reproducing-figure-1",
    "title": "Replication Study: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "We will start with figure 1 of the study which showed the number of chronic illnesses versus the algorithm-predicted risk, by race. The authors decided to plot this to understand if there were racial health disparities conditional on risk score.\n\n# Plotting figure 1 \n\n# get risk scores as percentile\ndf[\"risk_percentile\"] = (df[\"risk_score_t\"].rank(pct=True) * 100).round()\n\n# get mean number of chronic illnesses and store in new dataframe\ndf['mean_num_CI'] = df.groupby(['risk_percentile', 'race'])['gagne_sum_t'].transform('mean')\n\n# plotting\nfig, axs = plt.subplots(1,1)\naxs.set_title(\"Risk Score vs Number of Chronic Illnesses\")\naxs.set_xlabel(\"Algorithm Risk Score Percentile\")\naxs.set_ylabel(\"Mean Num of Chronic Illnesses\")\nsns.scatterplot(ax = axs,data=df,x='risk_percentile',y='mean_num_CI', hue='race')\n\n\n\n\n\n\n\n\nThis plot shows us that there is indeed a racial disparity between the algorithm’s risk score and the number of chronic illnesses a patient might have. We can see that black patients often have a higher number of chronic illnesses compared to white patients in the same risk score percentile. This means that if Patient A is Black, Patient B is White, and both patients have the exact same number of chronic illnesses, Patient A will likely receive a higher risk score from the algorithm and thus be more likely to be referred to the care management program."
  },
  {
    "objectID": "posts/replication-study/index.html#part-c-reproducing-figure-3",
    "href": "posts/replication-study/index.html#part-c-reproducing-figure-3",
    "title": "Replication Study: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "We can now look at reproducing figure 3 of the study which compared the mean total medical expenditure to percentile of the algorithm’s risk score as well as the mean total medical expenditure to the number of chronic illnesses a patient had. The authors decided to plot these comparisons to determine if there was any racial disparity in cost for healthcare and as a way to check if there was any disparity in the amount of care received between black and white patients.\n\n# Plotting Figure 3\n\n# get risk percentile\ndf['risk_percentile'] = (df['risk_score_t'].rank(pct=True)*100).round()\n\n# get mean expenditure\ndf[\"cost_by_RS\"] = df.groupby(['risk_percentile','race'])['cost_t'].transform('mean')\n\n# get mean cost of patient healthcare by number of chronic illness\ndf[\"cost_by_CI\"] = df.groupby(['gagne_sum_t','race'])['cost_t'].transform('mean')\n\n# plotting\nfig, axs = plt.subplots(1,2)\naxs[0].set_yscale('log')\naxs[0].set_xlabel(\"Algorithm Risk Score Percentile\")\naxs[0].set_ylabel(\"Mean Total Medical Expenditure\")\naxs[1].set_yscale('log')\naxs[1].set_xlabel(\"Number of Chronic Illnesses\")\naxs[1].set_ylabel(\" \")\nsns.scatterplot(ax=axs[0],data=df,x='risk_percentile',y='cost_by_RS', hue='race')\nsns.scatterplot(ax=axs[1],data=df,x='gagne_sum_t',y='cost_by_CI', hue='race')\n\n\n\n\n\n\n\n\nThese plots show us that as the number of chronic illnesses increase or as your risk score increases, the higher the cost of healthcare is for a patient. This is not particularly surprising since those with higher risk scores are generally less healthy and patients with more chronic illnesses will have higher care needs. However, it is important to note that these plots still show potential evidence of a racial disparity. Most patients have 5 or fewer chronic illnesses and we see that white patients tend to have higher costs than black patients if they have 5 or fewer chronic illnesses. We also see this trend reflected when comparing our risk score and medical expenditure. We find that white patients with lower risk scores tend to have higher healthcare expenditures than black patients. This phenomenon might point to potential racial bias in access to healthcare, where white patients with lower risk scores receive more treatment or care and in turn have higher medical expenditures."
  },
  {
    "objectID": "posts/replication-study/index.html#part-d-modeling-cost-disparity",
    "href": "posts/replication-study/index.html#part-d-modeling-cost-disparity",
    "title": "Replication Study: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "We can now try and quantify the disparity we just observed in patients that have 5 or fewer active chronic conditions.\n\n\nTo do so, we need to first determine the percentage of patients with 5 or fewer chronic conditions. If this percentage is small, it might not be worth focusing on these patients.\n\n# Determining percentage of patients with 5 or fewer active chronic conditions\npercentage = len(df.loc[df['gagne_sum_t'] &lt;= 5])/ len(df) * 100\npercentage\n\n95.53952115447689\n\n\n95.5% of patients have five or fewer active chronic conditions, so it does indeed make sense to focus on these patients since any racial disparities occurring within this subgroup of patients will be affecting an overwhelming majority of the people receiving healthcare.\nBecause medical costs can vary by orders of magnitude, we will want to look at the logorithm of the cost. However, in order to do this we also need to drop any patients who did not have any medical cost at all since log(0) is undefined.\n\n# Log-transform of cost\ndf_transform = (df.loc[df['cost_t'] &gt; 0]).copy()\ndf_transform['log_cost'] = np.log(df_transform['cost_t'])\n\nIt might also make it easier for us to do some modeling by creating a one-hot encoded column for the qualitative race variable. For us 1 means the patient is black and 0 means they are white.\nThen we will separate the data into predictor variables X and target variables Y. Our predictor variables are race and the number of active chronic conditions.\n\n# one-hot encoding of race\ndf_transform['race_binary'] = (df_transform['race'] == 'black') * 1\n\n# separation of data into predictor variables and target variables\ndf_X = df_transform[['race_binary','gagne_sum_t']]\ndf_y = df_transform['log_cost']\n\n\n\n\nWith our data prepped, we can now employ a linear regression model. However, our relationship is non-linear, so we will add polynomial features to our model. If we are going to do this, we must also determine how many polynomial features we will use. We can do this with some cross validation. For our purposes we will look from degree 1 to 20.\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\nLR = LinearRegression()\nscores = []\nfor i in range(20):\n  X_poly = add_polynomial_features(df_X,i)\n  #LR.fit(X_poly, df_y)\n  cross_valscore = cross_val_score(LR, X_poly, df_y)\n  score = (f\"degree {i+1}\",cross_valscore.mean())\n  scores.append(score) \n\nscores.sort(key=lambda x: x[1])\nscores\n\n[('degree 19', 0.10908769701215384),\n ('degree 20', 0.10940809669337344),\n ('degree 18', 0.1193285359520339),\n ('degree 17', 0.12852193502774437),\n ('degree 16', 0.1442007114428923),\n ('degree 3', 0.14537952700038775),\n ('degree 1', 0.14538846793594346),\n ('degree 2', 0.14538846793594346),\n ('degree 5', 0.146909632661908),\n ('degree 4', 0.14699336604386143),\n ('degree 6', 0.1473340656385787),\n ('degree 7', 0.14776094174784324),\n ('degree 15', 0.148069648494553),\n ('degree 8', 0.1480739986116853),\n ('degree 9', 0.14811660549076067),\n ('degree 10', 0.14820529985520275),\n ('degree 12', 0.14824297239083922),\n ('degree 13', 0.1482567134543757),\n ('degree 14', 0.14828133604468716),\n ('degree 11', 0.14830062340213399)]\n\n\nDegree 11 seems to have the highest score, so we will use that. Now we will run our model\n\n# final model\nnew_LR = LinearRegression()\nnew_df_X = add_polynomial_features(df_X, 11)\nnew_LR.fit(new_df_X, df_y)\n\nprint(new_df_X.columns)\nnew_LR.coef_\n\nIndex(['race_binary', 'gagne_sum_t', 'poly_1', 'poly_2', 'poly_3', 'poly_4',\n       'poly_5', 'poly_6', 'poly_7', 'poly_8', 'poly_9', 'poly_10'],\n      dtype='object')\n\n\narray([-2.66958964e-01,  6.12454343e-01,  6.12451935e-01, -1.49609200e+00,\n        9.79709805e-01, -3.45158893e-01,  7.23069132e-02, -9.40470735e-03,\n        7.65069325e-04, -3.78086925e-05,  1.03681896e-06, -1.20918699e-08])\n\n\nWe see that our race coefficient is the first column of our X and therefore is the first coefficient in our linear regression. Now we calculate our wb.\n\nwb = np.exp(new_LR.coef_[0])\nwb\n\n0.7657044921324225\n\n\nAfter running our model, we find that the cost incurred by black patients is about 76.5% that of white patients when considering patients with 5 or fewer chronic conditions. This finding supports the argument of Obermeyer et al. that black patients generate lower costs because they might not receive healthcare due to racial biases."
  },
  {
    "objectID": "posts/replication-study/index.html#discussion",
    "href": "posts/replication-study/index.html#discussion",
    "title": "Replication Study: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations",
    "section": "",
    "text": "Like the original study, I found that there exists a racial bias in this algorithm. Black patients with the same risk score as white patients often have lower risk scores, making it harder for them to be enrolled into an intensive care program. This disparity occurs because the algorithm uses healthcare spending as a proxy for health, but black patients often have lower healthcare costs because they have less access to healthcare. Therefore, like the journal article, we find evidence of racial disparity in the algorithm.\nThe algorithm fails the test of sufficiency outlined in the formal statistical discrimination criteria discussed by Barocas, Hardt, and Narayanan (2023). Sufficiency seeks to determine if we will get a certain outcome given a certain prediction. However, the algorithm very clearly gives different outcomes dependent on race, with black patients being more sick than white patients with the same risk score. THe algorithm might predict a black patient to not be qualified for the program, but a white patient will be, while indeed both might actually need to be enrolled. The algorithm could also fail a test of independence since the proxy variable (healthcare costs) affects the score, but healthcare costs are influenced by race and thus the score could be as well."
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Mihir Singh\n\n\nThe code for the logistic regression model can be found here.\nThis blog post first implements a logistic regression model with vanilla and spicy gradient descent. I then conducted experiments using this implementation. First, I found that vanilla gradient descent eventually converged on a weight that accurately separated the data it was classifying and that the loss of the model decreased monotonically. Then, I found that spicy gradient descent converged faster than vanilla gradient descent after adjusting my learning rate. Finally, I fit my model on training data with more features than data points and consequently found that I had overfit my model to my training data, receiving a much lower accuracy when applying the model over testing data. Through this process, I was able to gain a better understanding of logistic regression and gradient descent as well as recognize the disadvantages of overfitting a model and the advantages of gradient descent with momentum.\n\n\n\n# metadata\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\n\n\nIn order to perform any experiments, we will first generate data for a classification problem.\n\ndef classification_data(n_points = 300, noise = 0.3, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nBefore running any experiments, we should also take a look at the data we will be using. We can also create some functions that’ll help us evaluate our models.\n\n# function to plot data - from Professor Phil Chodrow\ndef plot_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# function to plot data - from Professor Phil Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n    \n# function to determine accuracy of logistic regression models\ndef accuracy(model, X, y):\n    return torch.mean((1.0*(model.predict(X) == y)))\n    \nfig, ax = plt.subplots(1, 1)\n\nX, y = classification_data()\nplot_data(X, y, ax)\n\n\n\n\n\n\n\n\nNow we can create our models and implement a gradient descent loop.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(100):\n    # add other stuff to e.g. keep track of the loss over time. \n    opt.step(X, y, alpha = 0.1, beta = 0.9)\n\n\n\nOur first experiment will be focused on vanilla gradient descent. If alpha is sufficiently small and beta = 0, then our logistic regression algorithm should converge to a weight vector w that looks correct visually.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nlossVec = []\nfor _ in range(2500):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = opt.step(X, y, alpha = 0.1, beta = 0)\n    lossVec.append(loss)\n\nAfter this loop, we can now plot our decision boundary and check how accurate our model is.\n\n# plot decision regions\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_data(X, y, ax)\nw = LR.w\ndraw_line(w, -0.5, 1.5, ax, color = \"black\", label = r\"$w^{(0)}$\")\n\n# find accuracy\nacc = accuracy(LR, X, y)\nprint(\"accuracy is:\", acc)\n\naccuracy is: tensor(0.9900)\n\n\n\n\n\n\n\n\n\nOur model is pretty accurate with an accuracy of 99%. Over more iterations it could even perfectly divide the data. Let’s see how the model improves over time by looking at the loss over each iteration of our training loop.\n\n# plotting loss\ndef loss_graph(loss_vec):\n    plt.plot(loss_vec, marker='o', linestyle='-')\n    plt.title('Loss Graph: Experiment 1')\n    plt.xlabel('Iteration')\n    plt.ylabel('Loss')\n    plt.show()\n    print(\"final loss: \", loss_vec[len(loss_vec)-1])\n\nloss_graph(loss_vec=lossVec)\n\n\n\n\n\n\n\n\nfinal loss:  tensor(0.0488)\n\n\nOur loss decreases over time and gets closer to converging to a loss of 0. This graph shows that our model is working properly and adjusting its weight accordingly in order to get the right answer.\n\n\n\nNow we can implement gradient descent with momentum into our training loop. If our beta = 0.9 and we modify our alpha, we should be able to see that our model will converge to the correct weight vector in fewer iterations than vanilla gradient descent.\n\n# train the model with momentum\nLRM = LogisticRegression() \noptM = GradientDescentOptimizer(LRM)\n\nlossVecM = []\nfor _ in range(2500):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = optM.step(X, y, alpha = 0.15, beta = 0.9)\n    lossVecM.append(loss)\n    \n# plot decision regions\nfig, ax = plt.subplots(1,1, figsize = (4, 4))\nplot_data(X, y, ax)\nw = LRM.w\ndraw_line(w, -0.5, 1.5, ax, color = \"black\", label = r\"$w^{(0)}$\")\n\n# find accuracy\nacc = accuracy(LRM, X, y)\nprint(\"accuracy is:\", acc)\n\naccuracy is: tensor(0.9900)\n\n\n\n\n\n\n\n\n\nWe see that after the same number of iterations, our model reaches the same accuracy.\n\nplt.plot(lossVec, color = \"red\", label='Vanilla')\nplt.plot(lossVecM, color = \"blue\", label='Momentum')\nplt.legend()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Vanilla vs Momentum Gradient Descent Loss Over Time\")\n\nText(0.5, 1.0, 'Vanilla vs Momentum Gradient Descent Loss Over Time')\n\n\n\n\n\n\n\n\n\nAfter modifying our alpha so that it is now 0.25, we find that we can indeed make it so that our model implementing momentum converges slightly faster than a model implementing vanilla gradient descent. We see this because our loss with the model implementing momentum decreases at a faster rate and thus converges sooner.\n\n\n\nOur last experiment will require data where the number of features is larger than the number of points. We will seek to overfit our model by obtaining 100% accuracy for our model on training data, but a much worse accuracy when we apply the model on test data.\n\n# generate training data\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n# generate testing data\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nWith our data generated, we can now begin training our model with the training data.\n\nLRE = LogisticRegression() \noptE = GradientDescentOptimizer(LRE)\n\n# training our model \nlossVecE = []\naccuracies_train = []\naccuracies_test = []\nfor _ in range(2500):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = optE.step(X_train, y_train, alpha = 0.25, beta = 0.9)\n    lossVecE.append(loss)\n    \n    # from accuracy function - need array though so slight modification\n    accuracies_train.append((1.0*(LRE.predict(X_train) == y_train)).mean())\n    accuracies_test.append((1.0*(LRE.predict(X_test) == y_test)).mean())   \n    \n# find accuracy over training data\nacc = accuracy(LRE, X_train, y_train)\nprint(\"training accuracy is:\", acc)\n\nacc = accuracy(LRE, X_test, y_test)\nprint(\"test accuracy is:\", acc)\n\ntraining accuracy is: tensor(1.)\ntest accuracy is: tensor(0.9200)\n\n\nOur model achieves perfect accuracy when classifying the training data. However, it is only 92% accurate when looking at the testing data. This is the lowest accuracy we have arrived at so far. We can take a look at the accuracy over time for each dataset.\n\nplt.plot(accuracies_train, color = \"red\", label='Training')\nplt.plot(accuracies_test, color = \"blue\", label='Testing')\nplt.legend()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Vanilla vs Momentum Gradient Descent Loss Over Time\")\n\nText(0.5, 1.0, 'Vanilla vs Momentum Gradient Descent Loss Over Time')\n\n\n\n\n\n\n\n\n\nFrom looking at this graph we see that our model underperformed when looking at the testing data at any iteration. Our results indicates that we have overfit our model on the training data and that our model will not be accurate when asked to classify other data.\n## Discussion Logistic regression works by continuously updating its weight(s) based upon a process of gradient descent. Vanilla gradient descent can be quite accurate, but if you adjust your learning rate and implement a process called gradient descent with momentum, you can converge on an accurate answer even faster. However, if you have more features than data points, you may overfit your model to your training data and consequently receive a much lower accuracy when applying the model over testing data. Through this blog post, I was able to learn about the implementation of logistic regression and gradient descent with momentum. I also recognized how you can overfit your model with training data with more features than data points and saw the benefits of spicy gradient descent. Through this blog post, I developed a better understanding of the popular data science technique that is logistic regression."
  },
  {
    "objectID": "posts/logistic-regression/index.html#abstract",
    "href": "posts/logistic-regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "The code for the logistic regression model can be found here.\nThis blog post first implements a logistic regression model with vanilla and spicy gradient descent. I then conducted experiments using this implementation. First, I found that vanilla gradient descent eventually converged on a weight that accurately separated the data it was classifying and that the loss of the model decreased monotonically. Then, I found that spicy gradient descent converged faster than vanilla gradient descent after adjusting my learning rate. Finally, I fit my model on training data with more features than data points and consequently found that I had overfit my model to my training data, receiving a much lower accuracy when applying the model over testing data. Through this process, I was able to gain a better understanding of logistic regression and gradient descent as well as recognize the disadvantages of overfitting a model and the advantages of gradient descent with momentum.\n\n\n\n# metadata\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiments",
    "href": "posts/logistic-regression/index.html#experiments",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "In order to perform any experiments, we will first generate data for a classification problem.\n\ndef classification_data(n_points = 300, noise = 0.3, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nBefore running any experiments, we should also take a look at the data we will be using. We can also create some functions that’ll help us evaluate our models.\n\n# function to plot data - from Professor Phil Chodrow\ndef plot_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# function to plot data - from Professor Phil Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n    \n# function to determine accuracy of logistic regression models\ndef accuracy(model, X, y):\n    return torch.mean((1.0*(model.predict(X) == y)))\n    \nfig, ax = plt.subplots(1, 1)\n\nX, y = classification_data()\nplot_data(X, y, ax)\n\n\n\n\n\n\n\n\nNow we can create our models and implement a gradient descent loop.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(100):\n    # add other stuff to e.g. keep track of the loss over time. \n    opt.step(X, y, alpha = 0.1, beta = 0.9)\n\n\n\nOur first experiment will be focused on vanilla gradient descent. If alpha is sufficiently small and beta = 0, then our logistic regression algorithm should converge to a weight vector w that looks correct visually.\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nlossVec = []\nfor _ in range(2500):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = opt.step(X, y, alpha = 0.1, beta = 0)\n    lossVec.append(loss)\n\nAfter this loop, we can now plot our decision boundary and check how accurate our model is.\n\n# plot decision regions\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_data(X, y, ax)\nw = LR.w\ndraw_line(w, -0.5, 1.5, ax, color = \"black\", label = r\"$w^{(0)}$\")\n\n# find accuracy\nacc = accuracy(LR, X, y)\nprint(\"accuracy is:\", acc)\n\naccuracy is: tensor(0.9900)\n\n\n\n\n\n\n\n\n\nOur model is pretty accurate with an accuracy of 99%. Over more iterations it could even perfectly divide the data. Let’s see how the model improves over time by looking at the loss over each iteration of our training loop.\n\n# plotting loss\ndef loss_graph(loss_vec):\n    plt.plot(loss_vec, marker='o', linestyle='-')\n    plt.title('Loss Graph: Experiment 1')\n    plt.xlabel('Iteration')\n    plt.ylabel('Loss')\n    plt.show()\n    print(\"final loss: \", loss_vec[len(loss_vec)-1])\n\nloss_graph(loss_vec=lossVec)\n\n\n\n\n\n\n\n\nfinal loss:  tensor(0.0488)\n\n\nOur loss decreases over time and gets closer to converging to a loss of 0. This graph shows that our model is working properly and adjusting its weight accordingly in order to get the right answer.\n\n\n\nNow we can implement gradient descent with momentum into our training loop. If our beta = 0.9 and we modify our alpha, we should be able to see that our model will converge to the correct weight vector in fewer iterations than vanilla gradient descent.\n\n# train the model with momentum\nLRM = LogisticRegression() \noptM = GradientDescentOptimizer(LRM)\n\nlossVecM = []\nfor _ in range(2500):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = optM.step(X, y, alpha = 0.15, beta = 0.9)\n    lossVecM.append(loss)\n    \n# plot decision regions\nfig, ax = plt.subplots(1,1, figsize = (4, 4))\nplot_data(X, y, ax)\nw = LRM.w\ndraw_line(w, -0.5, 1.5, ax, color = \"black\", label = r\"$w^{(0)}$\")\n\n# find accuracy\nacc = accuracy(LRM, X, y)\nprint(\"accuracy is:\", acc)\n\naccuracy is: tensor(0.9900)\n\n\n\n\n\n\n\n\n\nWe see that after the same number of iterations, our model reaches the same accuracy.\n\nplt.plot(lossVec, color = \"red\", label='Vanilla')\nplt.plot(lossVecM, color = \"blue\", label='Momentum')\nplt.legend()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Vanilla vs Momentum Gradient Descent Loss Over Time\")\n\nText(0.5, 1.0, 'Vanilla vs Momentum Gradient Descent Loss Over Time')\n\n\n\n\n\n\n\n\n\nAfter modifying our alpha so that it is now 0.25, we find that we can indeed make it so that our model implementing momentum converges slightly faster than a model implementing vanilla gradient descent. We see this because our loss with the model implementing momentum decreases at a faster rate and thus converges sooner.\n\n\n\nOur last experiment will require data where the number of features is larger than the number of points. We will seek to overfit our model by obtaining 100% accuracy for our model on training data, but a much worse accuracy when we apply the model on test data.\n\n# generate training data\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\n# generate testing data\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nWith our data generated, we can now begin training our model with the training data.\n\nLRE = LogisticRegression() \noptE = GradientDescentOptimizer(LRE)\n\n# training our model \nlossVecE = []\naccuracies_train = []\naccuracies_test = []\nfor _ in range(2500):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = optE.step(X_train, y_train, alpha = 0.25, beta = 0.9)\n    lossVecE.append(loss)\n    \n    # from accuracy function - need array though so slight modification\n    accuracies_train.append((1.0*(LRE.predict(X_train) == y_train)).mean())\n    accuracies_test.append((1.0*(LRE.predict(X_test) == y_test)).mean())   \n    \n# find accuracy over training data\nacc = accuracy(LRE, X_train, y_train)\nprint(\"training accuracy is:\", acc)\n\nacc = accuracy(LRE, X_test, y_test)\nprint(\"test accuracy is:\", acc)\n\ntraining accuracy is: tensor(1.)\ntest accuracy is: tensor(0.9200)\n\n\nOur model achieves perfect accuracy when classifying the training data. However, it is only 92% accurate when looking at the testing data. This is the lowest accuracy we have arrived at so far. We can take a look at the accuracy over time for each dataset.\n\nplt.plot(accuracies_train, color = \"red\", label='Training')\nplt.plot(accuracies_test, color = \"blue\", label='Testing')\nplt.legend()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Vanilla vs Momentum Gradient Descent Loss Over Time\")\n\nText(0.5, 1.0, 'Vanilla vs Momentum Gradient Descent Loss Over Time')\n\n\n\n\n\n\n\n\n\nFrom looking at this graph we see that our model underperformed when looking at the testing data at any iteration. Our results indicates that we have overfit our model on the training data and that our model will not be accurate when asked to classify other data.\n## Discussion Logistic regression works by continuously updating its weight(s) based upon a process of gradient descent. Vanilla gradient descent can be quite accurate, but if you adjust your learning rate and implement a process called gradient descent with momentum, you can converge on an accurate answer even faster. However, if you have more features than data points, you may overfit your model to your training data and consequently receive a much lower accuracy when applying the model over testing data. Through this blog post, I was able to learn about the implementation of logistic regression and gradient descent with momentum. I also recognized how you can overfit your model with training data with more features than data points and saw the benefits of spicy gradient descent. Through this blog post, I developed a better understanding of the popular data science technique that is logistic regression."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Mihir Singh\n\n\nBy developing a model to classify the Palmer penguins, there can be a more accurate species identification of penguins given a certain set of features. Comparing the different features in the Palmer penguin data set, it was determined that the culmen length, culmen depth, and island are useful features for determining a penguin’s species. Different types of classification models were also compared (logistic regress, decision tree, and SVC). It was determined that a logistic regression model looking at a penguins island of residence, culmen depth, and culmen depth would be a highly accurate model. When it was applied to the testing data, it received an accuracy score of %100.\nFirst we must access the training data. We can also import any libraries we might need later on.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nThis code prepares qualitative columns in the data. Provided by Professor Phil Chodrow.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNo we can then create visualizations that give us some new understanding about the Palmer Penguins.\nWe can first look to see if there is any difference in body mass by island. To see this, we can create a box plot with the body mass being on the y-axis and the islands being the discrete categories making up the x-axis.\n\np1 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", )\n\n\n\n\n\n\n\n\nIn p1 we can see that penguins on Biscoe island are generally much larger in body mass than penguins on other islands.\nWe can then also look to see if the difference is holds within species. That is, is a Chinstrap penguin on Biscoe island generally bigger than a Chinstrap penguin on Torgersen island, or is it that a certain species of penguin dominates Biscoe island.\n\np2 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = \"Species\" ,palette=[\"r\", \"g\", 'b'])\n\n\n\n\n\n\n\n\nIn p2 we find that a reason that the penguins on Biscoe Island are heavier may be because of its population of Gentoo Penguins. There are no other islands with a Gentoo penguin population, and they seem to be much heavier than other penguin species.\nWe can check this by looking at the mean body mass of penguins across species.\n\nsummaryTable = train.groupby(['Species']).aggregate(avgMass=('Body Mass (g)', 'mean'))\nprint(summaryTable)\n\n                                               avgMass\nSpecies                                               \nAdelie Penguin (Pygoscelis adeliae)        3718.487395\nChinstrap penguin (Pygoscelis antarctica)  3743.421053\nGentoo penguin (Pygoscelis papua)          5039.948454\n\n\nIndeed, we see that Gentoo penguins are, on average, much heavier than other penguins. So it makes sense to conclude that their prescence on Biscoe Island and abscence on other islands might be a big reason the average penguin on Biscoe Island is larger than the average penguin on other islands.\nIt might also be interesting to see if penguins with a bigger culmen length or depth are heavier. Again we can plot body mass and culmen length/depth to see this relationship.\n\np3 = sns.boxplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\np3 = sns.boxplot(train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\")\np3.set_xticks([0,10,20,30,40,50,60,70])\n\n\n\n\n\n\n\n\nWe see that as culmen length increases, the mass of the penguin generally increases, but the same is not true for culmen depth. In fact, the body mass is highest when the culmen depth reaches around 15-16 mm. These measurements line up with the culmen depth range of Chinstrap penguins so we can conclude that culmen depth doesn’t really have that much of an effect on body mass.\n\n\n\nFrom our visualizations we can infer that body mass and the island the penguin is on might be useful features. However, we should probably look for 3 good features.\nWe can start by eliminating features with low variance.\n\nfrom sklearn.feature_selection import VarianceThreshold\nX = X_train\n\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nnewTraining = sel.fit_transform(X)\n\nFirst, we can iterate through every qualitative and quantitative variable and create different combinations of features to test.\nThen we can start preparing models. The models I will use are a logistic regression, decision tree, and svc classifier.\nFinally, we can cross-validate these models.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import cross_val_score\n\n# values to keep track of best score from each classifier\nLRBest = 0\nDTBest = 0\nSVCBest = 0\nLRBestCols = []\nDTBestCols = []\nSVCBestCols = []\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    \n    # logistic regression\n    LR = LogisticRegression(max_iter=5000)\n    LR.fit(X_train[cols], y_train)\n    scoreLR = LR.score(X_train[cols], y_train)\n    if scoreLR &gt; LRBest:\n      LRBest = scoreLR\n      LRBestCols = cols\n    \n    # decision tree\n    DT = DecisionTreeClassifier(max_depth=20)\n    DT.fit(X_train[cols], y_train)\n    scoreDT = DT.score(X_train[cols], y_train)\n    if scoreDT &gt; DTBest:\n      DTBest = scoreDT\n      DTBestCols = cols\n    \n    # SVC\n    SVCModel = SVC(gamma='auto')\n    SVCModel.fit(X_train[cols], y_train)\n    scoreSVC = SVCModel.score(X_train[cols], y_train)\n    if scoreSVC &gt; SVCBest:\n      SVCBest = scoreSVC\n      SVCBestCols = cols\n\nprint(LRBest, \" \", LRBestCols)\nprint(DTBest, \" \", DTBestCols)\nprint(SVCBest, \" \", SVCBestCols)\n\n# cross validate models\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\nprint((cv_scores_LR).mean())\n\ncv_scores_DT = cross_val_score(DT, X_train, y_train, cv=5)\nprint((cv_scores_DT).mean())\n\ncv_scores_SVC = cross_val_score(SVCModel, X_train, y_train, cv=5)\nprint((cv_scores_SVC).mean())\n\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0   ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n0.9569381598793363\n0.6407239819004525\n\n\nWhile we do find that the decision tree classifier does have a After cross-validating the models, it seems that a logistic regression is best. We can now look at creating a logistic regression model with the three best features for it (which were Island, Culmen Length, and Culmen Depth).\nWe found these three best features by running through every possible combination of three features (with one qualitative and 2 quantitative) and keep track of which combination led to the high model score.\n\n\n\nWith our models and features selected, we can test our decisions/model.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\n\n\nWe can plot the decision regions of our classifiers to get a better understanding of the conclusions our model drew.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at a confusion matrix to give us a better understanding of our model’s accuracy.\n\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = LR.predict(X_test[cols])\ny_test_pred\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nFrom this we see that we correctly predict every penguin. Since there are no values outside the diagonal of the matrix, we don’t have any false predictions. Given that we use island as a feature, we will probably see more errors predicting Adelie penguins as Chinstrap or Gentoo. This is because the Adelie penguin is the only penguin species that is present on every island so it is probably easiest for our model to confuse it for another species.\n\n\n\nA logistic regression model trained by looking at the Palmer penguin dataset features of culmen length, culmen depth, and home island was highly accurate, getting an accuracy score of %100 on the test data.\nBy finding this model, I learned about the importance of conducting thorough research on your data. Much of the initial work did not involve creating models but exploring the features and characteristics of the dataset. Only after this exploration was I able to develop an intuition for what features might be useful in creating a model. However, just exploring the data is not enough to create a good model, cross-validation and eliminating features with low variance were also important techniques employed in this process, guiding against overfitting and helping narrow down on useful features respectively."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#abstract",
    "href": "posts/classifying-palmer-penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "By developing a model to classify the Palmer penguins, there can be a more accurate species identification of penguins given a certain set of features. Comparing the different features in the Palmer penguin data set, it was determined that the culmen length, culmen depth, and island are useful features for determining a penguin’s species. Different types of classification models were also compared (logistic regress, decision tree, and SVC). It was determined that a logistic regression model looking at a penguins island of residence, culmen depth, and culmen depth would be a highly accurate model. When it was applied to the testing data, it received an accuracy score of %100.\nFirst we must access the training data. We can also import any libraries we might need later on.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#data-preparation",
    "href": "posts/classifying-palmer-penguins/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This code prepares qualitative columns in the data. Provided by Professor Phil Chodrow.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#explore",
    "href": "posts/classifying-palmer-penguins/index.html#explore",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "No we can then create visualizations that give us some new understanding about the Palmer Penguins.\nWe can first look to see if there is any difference in body mass by island. To see this, we can create a box plot with the body mass being on the y-axis and the islands being the discrete categories making up the x-axis.\n\np1 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", )\n\n\n\n\n\n\n\n\nIn p1 we can see that penguins on Biscoe island are generally much larger in body mass than penguins on other islands.\nWe can then also look to see if the difference is holds within species. That is, is a Chinstrap penguin on Biscoe island generally bigger than a Chinstrap penguin on Torgersen island, or is it that a certain species of penguin dominates Biscoe island.\n\np2 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = \"Species\" ,palette=[\"r\", \"g\", 'b'])\n\n\n\n\n\n\n\n\nIn p2 we find that a reason that the penguins on Biscoe Island are heavier may be because of its population of Gentoo Penguins. There are no other islands with a Gentoo penguin population, and they seem to be much heavier than other penguin species.\nWe can check this by looking at the mean body mass of penguins across species.\n\nsummaryTable = train.groupby(['Species']).aggregate(avgMass=('Body Mass (g)', 'mean'))\nprint(summaryTable)\n\n                                               avgMass\nSpecies                                               \nAdelie Penguin (Pygoscelis adeliae)        3718.487395\nChinstrap penguin (Pygoscelis antarctica)  3743.421053\nGentoo penguin (Pygoscelis papua)          5039.948454\n\n\nIndeed, we see that Gentoo penguins are, on average, much heavier than other penguins. So it makes sense to conclude that their prescence on Biscoe Island and abscence on other islands might be a big reason the average penguin on Biscoe Island is larger than the average penguin on other islands.\nIt might also be interesting to see if penguins with a bigger culmen length or depth are heavier. Again we can plot body mass and culmen length/depth to see this relationship.\n\np3 = sns.boxplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\np3 = sns.boxplot(train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\")\np3.set_xticks([0,10,20,30,40,50,60,70])\n\n\n\n\n\n\n\n\nWe see that as culmen length increases, the mass of the penguin generally increases, but the same is not true for culmen depth. In fact, the body mass is highest when the culmen depth reaches around 15-16 mm. These measurements line up with the culmen depth range of Chinstrap penguins so we can conclude that culmen depth doesn’t really have that much of an effect on body mass."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#choosing-features-and-models",
    "href": "posts/classifying-palmer-penguins/index.html#choosing-features-and-models",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "From our visualizations we can infer that body mass and the island the penguin is on might be useful features. However, we should probably look for 3 good features.\nWe can start by eliminating features with low variance.\n\nfrom sklearn.feature_selection import VarianceThreshold\nX = X_train\n\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nnewTraining = sel.fit_transform(X)\n\nFirst, we can iterate through every qualitative and quantitative variable and create different combinations of features to test.\nThen we can start preparing models. The models I will use are a logistic regression, decision tree, and svc classifier.\nFinally, we can cross-validate these models.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import cross_val_score\n\n# values to keep track of best score from each classifier\nLRBest = 0\nDTBest = 0\nSVCBest = 0\nLRBestCols = []\nDTBestCols = []\nSVCBestCols = []\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    \n    # logistic regression\n    LR = LogisticRegression(max_iter=5000)\n    LR.fit(X_train[cols], y_train)\n    scoreLR = LR.score(X_train[cols], y_train)\n    if scoreLR &gt; LRBest:\n      LRBest = scoreLR\n      LRBestCols = cols\n    \n    # decision tree\n    DT = DecisionTreeClassifier(max_depth=20)\n    DT.fit(X_train[cols], y_train)\n    scoreDT = DT.score(X_train[cols], y_train)\n    if scoreDT &gt; DTBest:\n      DTBest = scoreDT\n      DTBestCols = cols\n    \n    # SVC\n    SVCModel = SVC(gamma='auto')\n    SVCModel.fit(X_train[cols], y_train)\n    scoreSVC = SVCModel.score(X_train[cols], y_train)\n    if scoreSVC &gt; SVCBest:\n      SVCBest = scoreSVC\n      SVCBestCols = cols\n\nprint(LRBest, \" \", LRBestCols)\nprint(DTBest, \" \", DTBestCols)\nprint(SVCBest, \" \", SVCBestCols)\n\n# cross validate models\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\nprint((cv_scores_LR).mean())\n\ncv_scores_DT = cross_val_score(DT, X_train, y_train, cv=5)\nprint((cv_scores_DT).mean())\n\ncv_scores_SVC = cross_val_score(SVCModel, X_train, y_train, cv=5)\nprint((cv_scores_SVC).mean())\n\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0   ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n0.9569381598793363\n0.6407239819004525\n\n\nWhile we do find that the decision tree classifier does have a After cross-validating the models, it seems that a logistic regression is best. We can now look at creating a logistic regression model with the three best features for it (which were Island, Culmen Length, and Culmen Depth).\nWe found these three best features by running through every possible combination of three features (with one qualitative and 2 quantitative) and keep track of which combination led to the high model score."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#testing",
    "href": "posts/classifying-palmer-penguins/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "With our models and features selected, we can test our decisions/model.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test)\n\n1.0"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "We can plot the decision regions of our classifiers to get a better understanding of the conclusions our model drew.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \nplot_regions(LR, X_train[cols], y_train)"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#confusion-matrix",
    "href": "posts/classifying-palmer-penguins/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "We can also take a look at a confusion matrix to give us a better understanding of our model’s accuracy.\n\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = LR.predict(X_test[cols])\ny_test_pred\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nFrom this we see that we correctly predict every penguin. Since there are no values outside the diagonal of the matrix, we don’t have any false predictions. Given that we use island as a feature, we will probably see more errors predicting Adelie penguins as Chinstrap or Gentoo. This is because the Adelie penguin is the only penguin species that is present on every island so it is probably easiest for our model to confuse it for another species."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#discussion",
    "href": "posts/classifying-palmer-penguins/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "A logistic regression model trained by looking at the Palmer penguin dataset features of culmen length, culmen depth, and home island was highly accurate, getting an accuracy score of %100 on the test data.\nBy finding this model, I learned about the importance of conducting thorough research on your data. Much of the initial work did not involve creating models but exploring the features and characteristics of the dataset. Only after this exploration was I able to develop an intuition for what features might be useful in creating a model. However, just exploring the data is not enough to create a good model, cross-validation and eliminating features with low variance were also important techniques employed in this process, guiding against overfitting and helping narrow down on useful features respectively."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study: Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‘Optimal’ Decision-Making\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNewton’s Method for Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/decision-making/index.html",
    "href": "posts/decision-making/index.html",
    "title": "‘Optimal’ Decision-Making",
    "section": "",
    "text": "Mihir Singh\n\n\nThe purpose of this blog post was to create a decision system that maximizes the profit a bank will earn as well investigate the implications of this decision system on individual borrowers. I first investigated the data to find some general underlying trends before building a model that could predict the probability someone would repay a loan (which is the score for an individual). With the model built, I then found the threshold probability of repayment (ie threshold score) for the model’s prediction that would optimize the profit of the bank. I then investigated the implications of this threshold on individual borrowers by comparing score to age and income level as well as looking at the variation in the approval and repayment rates of different types of loans. The model maximized profit with a a threshold at a score of 0.53 and ultimately earned the bank $7697437.56. The model’s score was influenced by factors such as age, income level, and loan type."
  },
  {
    "objectID": "posts/decision-making/index.html#abstract",
    "href": "posts/decision-making/index.html#abstract",
    "title": "‘Optimal’ Decision-Making",
    "section": "",
    "text": "The purpose of this blog post was to create a decision system that maximizes the profit a bank will earn as well investigate the implications of this decision system on individual borrowers. I first investigated the data to find some general underlying trends before building a model that could predict the probability someone would repay a loan (which is the score for an individual). With the model built, I then found the threshold probability of repayment (ie threshold score) for the model’s prediction that would optimize the profit of the bank. I then investigated the implications of this threshold on individual borrowers by comparing score to age and income level as well as looking at the variation in the approval and repayment rates of different types of loans. The model maximized profit with a a threshold at a score of 0.53 and ultimately earned the bank $7697437.56. The model’s score was influenced by factors such as age, income level, and loan type."
  },
  {
    "objectID": "posts/decision-making/index.html#exploring-the-data",
    "href": "posts/decision-making/index.html#exploring-the-data",
    "title": "‘Optimal’ Decision-Making",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nNow that we have some data to work with, let’s try and find some patterns. We can start by creating a summary table comparing the avereage loan amount by homeownership status of an individual.\n\nsummaryTable = df_train.groupby(['person_home_ownership']).aggregate(Loan_Average =('loan_amnt', 'mean'))\nprint(summaryTable)\n\n                       Loan_Average\nperson_home_ownership              \nMORTGAGE               10562.137462\nOTHER                  11235.795455\nOWN                     8978.912626\nRENT                    8843.507973\n\n\nWe see here that those who have a mortgage have the highest average loan amount. This could be due to the fact that those with a mortgage have a higher income and are more likely to be approved for a larger loan. However, the largest loan average was for those whose home ownership status was ‘other.’ This could be students with loans for educational purposes living in college dorms.\nWe can also compare an individual’s age and their loan amount. To do this we will group individuals into quartiles.\n\n# there is someone 144 years old - might get rid of that outlier\ndf_train_age_copy = df_train.copy()\ndf_train_age_copy = df_train_age_copy[df_train_age_copy['person_age'] != 144]\n\n# group individuals into quartiles by age\nquartiles = df_train_age_copy['person_age'].quantile([0.25, 0.5, 0.75])\nfirstQuartile = f\"{df_train_age_copy['person_age'].min()} to {quartiles[0.25]}\"\nsecondQuartile = f\"{quartiles[0.25]} to {quartiles[0.50]}\"\nthirdQuartile = f\"{quartiles[0.50]} to {quartiles[0.75]}\"\nfourthQuartile = f\"{quartiles[0.75]} to {df_train_age_copy['person_age'].max()}\"\n\n# separate individuals into quartiles\ndf_train_age_copy['age_quartile'] = pd.qcut(df_train['person_age'], q = [0, 0.25, 0.5, 0.75, 1], labels = [firstQuartile, secondQuartile, thirdQuartile, fourthQuartile])\n\n# plot\nsns.boxplot(df_train_age_copy, x='age_quartile', y='loan_amnt')\nprint(\"average loan amt by\", df_train_age_copy.groupby('age_quartile')['loan_amnt'].mean())\n\naverage loan amt by age_quartile\n20 to 23.0      8790.989412\n23.0 to 26.0    9739.076064\n26.0 to 30.0    9967.734319\n30.0 to 123     9889.924230\nName: loan_amnt, dtype: float64\n\n\n/Users/mihirsingh/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/var/folders/fv/_v7myjl904983sbb39qbkzw40000gn/T/ipykernel_32471/2138611228.py:17: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  print(\"average loan amt by\", df_train_age_copy.groupby('age_quartile')['loan_amnt'].mean())\n\n\n\n\n\n\n\n\n\nWhile there is a slight variation between the loan amounts and age, it is not that strong. It is also interesting to note that the data includes someone who is 144 years old (though this is probably an error). This outlier was removed from the data for this plot.\nWe might not have been able to find a strong correlation between age and loan amount, but we might have better luck looking at whether those who have been employed for a longer period of time are more likely to have a large line of credit. Like before, we will group individuals into quartiles based on employment length.\n\n# there is someone working for 144 years - might get rid of that outlier\ndf_train_work_copy = df_train.copy()\ndf_train_work_copy = df_train_work_copy[df_train_work_copy['person_emp_length'] != 144]\n\n# group individuals into quartiles by age\nquartiles = df_train_work_copy['person_emp_length'].quantile([0.25, 0.5, 0.75])\nfirstQuartile = f\"{df_train_work_copy['person_emp_length'].min()} to {quartiles[0.25]}\"\nsecondQuartile = f\"{quartiles[0.25]} to {quartiles[0.50]}\"\nthirdQuartile = f\"{quartiles[0.50]} to {quartiles[0.75]}\"\nfourthQuartile = f\"{quartiles[0.75]} to {df_train_work_copy['person_emp_length'].max()}\"\n\n# separate individuals into quartiles\ndf_train_work_copy['employment_length_quartile'] = pd.qcut(df_train_work_copy['person_emp_length'], q = [0, 0.25, 0.5, 0.75, 1], labels = [firstQuartile, secondQuartile, thirdQuartile, fourthQuartile])\n\n# plot\nsns.boxplot(df_train_work_copy, x='employment_length_quartile', y='loan_amnt')\nprint(\"average loan amt by employment length\", df_train_work_copy.groupby('employment_length_quartile')['loan_amnt'].mean())\ndf_train_work_copy['employment_length_quartile'] = pd.qcut(df_train_work_copy['person_emp_length'], q = [0, 0.25, 0.5, 0.75, 1], labels = [firstQuartile, secondQuartile, thirdQuartile, fourthQuartile])\nprint(df_train_work_copy['person_emp_length'].max())\n\naverage loan amt by employment length employment_length_quartile\n0.0 to 2.0       8940.104406\n2.0 to 4.0       9423.175327\n4.0 to 7.0       9783.332000\n7.0 to 123.0    10805.937675\nName: loan_amnt, dtype: float64\n123.0\n\n\n/Users/mihirsingh/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/var/folders/fv/_v7myjl904983sbb39qbkzw40000gn/T/ipykernel_32471/1445965932.py:17: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  print(\"average loan amt by employment length\", df_train_work_copy.groupby('employment_length_quartile')['loan_amnt'].mean())\n\n\n\n\n\n\n\n\n\nLoan amount does slightly increase over time as employment length increased. This increase is generally pretty small, but is most prominent when someone has been working for 7+ years."
  },
  {
    "objectID": "posts/decision-making/index.html#finding-a-threshold",
    "href": "posts/decision-making/index.html#finding-a-threshold",
    "title": "‘Optimal’ Decision-Making",
    "section": "Finding a Threshold",
    "text": "Finding a Threshold\nNow that we have a weight vector and a model, we need to choose a threshold to determine whether a person is likely to default on a loan. We will use the following assumptions about how a bank makes/loses money:\n\nif a loan is repaid in full, profit = loan_amnt * (1+0.25*loan_int_rate)^10 - loan_amnt\nif an individual defaults, profit = loan_amnt * (1+0.25loan_int_rate)^3 - 1.7loan_amnt\n\nWe will use these assumptions to determine the optimal threshold for our model.\nFirst, we should look at an Receiver Operating Characteristic (ROC) curve to visually see our model’s performance at predicting whether or not an individual will default.\n\n# computing and plotting ROC curve - from Professor Phil Chodrow\ndef linear_score(X, w):\n    return X@w\n\ns = linear_score(df_model[LRBestCols], w)\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\nnum_thresholds = 101\n\nFPR = np.zeros(num_thresholds)\nTPR = np.zeros(num_thresholds)\n\nT = np.linspace(s.min()-0.1, s.max()+0.1, num_thresholds)\ns    = linear_score(df_model[LRBestCols], w)\n\nfor i in range(num_thresholds):\n    t = T[i]\n    preds    = s &gt;= t\n    FPR[i]   = ((preds == 1) & (df_model['loan_status'] == 0)).sum() / (df_model['loan_status']  == 0).sum()\n    TPR[i]   = ((preds == 1) & (df_model['loan_status'] == 1)).sum() / (df_model['loan_status']  == 1).sum()\n\nax.plot(FPR, TPR, color = \"black\")\nax.plot([0,1], [0,1], linestyle=\"--\", color = \"grey\")\nax.set_aspect('equal')\n\nlabs = ax.set(xlabel = \"False Positive Rate\", ylabel = \"True Positive Rate\", title = \"ROC Curve\")\n\n\n\n\n\n\n\n\nThe ROC curve does an ok job at predicting whether an individual will default. Ideally we would see the solid line curve farther towards the top left of the plot.\nNow we can look at the expected gain as a function of the threshold. This curve will help us determine which threshold is optimal for our model.\nHowever, in order to first do this, we need to calculate the profit the bank might make on a loan. This itself depends on the probability someone will repay or default.\n\n# get probability of being paid and add to df\nprobabilityPaid = LR.predict_proba(df_model[LRBestCols])\ndf_model['probabilityPaid'] = probabilityPaid[:,0].tolist()\n\nNow that we have the probability that someone will repay. We can calculate the profit for the bank for each individual. We will only look at people whose probability is above a certain threshold. From there, we will look and see if they defaulted or not and calculate the profit based off these values.\n\ndef findProfitByThreshold(t, df_model):\n    aboveThreshold = df_model[df_model['probabilityPaid'] &gt; t].copy()\n    paid = 0\n    default = 0\n    \n    for i in range(aboveThreshold.shape[0]):\n        if aboveThreshold.iloc[i]['loan_status'] == 0:\n            loanAmnt = aboveThreshold.iloc[i]['loan_amnt']\n            loanInt = aboveThreshold.iloc[i]['loan_int_rate']\n            paid += loanAmnt*(1 + 0.25*(loanInt/100))**10 - loanAmnt \n        else:\n            loanAmnt = aboveThreshold.iloc[i]['loan_amnt']\n            loanInt = aboveThreshold.iloc[i]['loan_int_rate']\n            default += loanAmnt*(1 + 0.25*(loanInt/100))**3 - 1.7*loanAmnt \n    return paid + default\n\n\nthresholds = np.linspace(0, 1, 100)\nprofits = []\nfor t in thresholds:\n    profits.append(findProfitByThreshold(t, df_model))\nplt.plot(thresholds, profits)\n\n\n\n\n\n\n\n\nWe see that our make profit occurs when we have a threshold around the 0.5 range. We can find the exact value by finding the maximum value of the curve.\n\n# find best threshold\nprint(f\"Max profit of ${max(profits)} at threshold = {profits.index(max(profits))/100}\")\nprint(f\"Max profit/borrower of ${max(profits)/len(df_model[df_model['probabilityPaid'] &gt;= 0.53])}\")\n\nMax profit of $32406222.49662961 at threshold = 0.53\nMax profit/borrower of $1566.9562640408883\n\n\nSo our best threshold is when the probability of repayment is 0.53. This is the point where the bank will make the most profit, earning $32406222.49."
  },
  {
    "objectID": "posts/decision-making/index.html#evaluating-our-model-from-the-banks-perspective",
    "href": "posts/decision-making/index.html#evaluating-our-model-from-the-banks-perspective",
    "title": "‘Optimal’ Decision-Making",
    "section": "Evaluating Our Model from the Bank’s Perspective",
    "text": "Evaluating Our Model from the Bank’s Perspective\nWe will now evaluate our model on some test data. To do so, we will use the threshold we determined earlier of t=0.53. We will also have to encode the data before we can run our model.\n\n# import data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\n# remove loan  grade col\ndf_tc = df_test.copy()\ndf_tc.drop(columns = 'loan_grade', inplace = True)\ndf_tc = df_tc[df_tc['person_age'] != 144]\n\n# need to encode features so we can actually use logistic regression\n# cb, loan intent, encoding type of home ownership\nencoder = LabelEncoder()\nencoder.fit(df_tc['cb_person_default_on_file'])\ndf_tc['cb_person_default_on_file'] = encoder.transform(df_tc['cb_person_default_on_file'])\nencoder.fit(df_tc['person_home_ownership'])\ndf_tc['person_home_ownership'] = encoder.transform(df_tc['person_home_ownership'])\nencoder.fit(df_tc['loan_intent'])\ndf_tc['loan_intent'] = encoder.transform(df_tc['loan_intent'])\ndf_tc = df_tc.dropna()\ndf_tc['person_emp_length'] = df_tc['person_emp_length'].astype(int)\ndf_tc.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n21\n42000\n3\n5\n5\n1000\n15.58\n1\n0.02\n0\n4\n\n\n1\n32\n51000\n0\n2\n0\n15000\n11.36\n0\n0.29\n0\n9\n\n\n2\n35\n54084\n3\n2\n0\n3000\n12.61\n0\n0.06\n0\n6\n\n\n3\n28\n66300\n0\n11\n3\n12000\n14.11\n1\n0.15\n0\n6\n\n\n4\n22\n70550\n3\n0\n3\n7000\n15.88\n1\n0.08\n0\n3\n\n\n\n\n\n\n\nWith the data all set up, we can use our model.\n\n# make predictions on probability individual will repay\nprobabilityPaid = LR.predict_proba(df_tc[LRBestCols])\ndf_tc['probabilityPaid'] = probabilityPaid[:,0].tolist()\n\n# find profit\nprofit = findProfitByThreshold(0.53,df_tc)\nprint(f\"profit is: {profit}\")\nprint(f\"profit per borrower is: {profit/len(df_tc[df_tc['probabilityPaid'] &gt;= 0.53])}\")\n\nprofit is: 7697437.566609849\nprofit per borrower is: 1500.4751591832064\n\n\nAt our threshold 0f 0.53, the bank will make a profit of $7697437.56 with a profit per borrower of $1500."
  },
  {
    "objectID": "posts/decision-making/index.html#evaluating-our-model-from-the-borrowers-perspective",
    "href": "posts/decision-making/index.html#evaluating-our-model-from-the-borrowers-perspective",
    "title": "‘Optimal’ Decision-Making",
    "section": "Evaluating Our Model from the Borrower’s Perspective",
    "text": "Evaluating Our Model from the Borrower’s Perspective\nAfter looking at the performance of our model from the bank’s perspective, we will now turn towards an evaluation of our model from the perspective of prospective borrowers.\n\nFirst we look at if it is more difficult for people in certain age groups to access credit under our model.\n\n\n# age group vs access to credit (model evaluation of probability individual will repay)\n# there is someone 144 years old - might get rid of that outlier\ndf_tc = df_tc[df_tc['person_age'] != 144]\n\n# group individuals into quartiles by age\nquartiles = df_tc['person_age'].quantile([0.25, 0.5, 0.75])\nfirstQuartile = f\"{df_tc['person_age'].min()} to {quartiles[0.25]}\"\nsecondQuartile = f\"{quartiles[0.25]} to {quartiles[0.50]}\"\nthirdQuartile = f\"{quartiles[0.50]} to {quartiles[0.75]}\"\nfourthQuartile = f\"{quartiles[0.75]} to {df_tc['person_age'].max()}\"\n\n# separate individuals into quartiles\ndf_tc['age_quartile'] = pd.qcut(df_tc['person_age'], q = [0, 0.25, 0.5, 0.75, 1], labels = [firstQuartile, secondQuartile, thirdQuartile, fourthQuartile])\n\n# plot\nsns.boxplot(x=df_tc['age_quartile'], y=df_tc['probabilityPaid'])\nplt.title('Age Group vs Access to Credit')\nplt.ylabel('Score')\n\n/Users/mihirsingh/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\nText(0, 0.5, 'Score')\n\n\n\n\n\n\n\n\n\nIt seems that those who are younger are more likely to be denied credit. This could be due to the fact that younger people have less credit history and are more likely to default on a loan. Overall, there was not much of a different in access to credit from ages 23-70.\n\nWe will now look at if it is more difficult to get a loan in order to pay for medical expenses. Here, we will look at approval rate by loan type and the percent each type of loan was repaid.\n\n\n# unencode loan intent\ndf_compare = df_tc.copy()\ndf_compare['loan_intent'] = encoder.inverse_transform(df_tc['loan_intent'])\n\n# approval by loan type\ndf_compare['approved'] = df_compare['probabilityPaid'] &gt; 0.53\napproved = df_compare.groupby('loan_intent')['approved'].mean()\nprint(type(approved))\n# repaid by loan type\ndf_compare['repaid'] = df_compare['loan_status'] == 0\nrepaid = df_compare.groupby('loan_intent')['repaid'].mean()\n\n# formatting\ncomparison = pd.DataFrame({'approved': approved, 'repaid': repaid})\ncomparison.reset_index(inplace=True)\ncomparison\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\n\n\n\n\n\n\n\nloan_intent\napproved\nrepaid\n\n\n\n\n0\nDEBTCONSOLIDATION\n0.884956\n0.712389\n\n\n1\nEDUCATION\n0.897109\n0.832483\n\n\n2\nHOMEIMPROVEMENT\n0.933442\n0.750000\n\n\n3\nMEDICAL\n0.876048\n0.715750\n\n\n4\nPERSONAL\n0.890782\n0.779559\n\n\n5\nVENTURE\n0.903527\n0.853734\n\n\n\n\n\n\n\nIt seems that loans for medical expenses are approved at the lowest rate (although not by too much compared to other loan types). However, loans for medical expenses are repaid at one of the lowest rates. Meanwhile, loans for education or a business venture are more likely to be approved and also more likely to be repaid.\n\nFinally, we will look at if a person’s income level impacts how easy it is for them to access credit under our model. To do this, we will look at the score our model generally gives an individual by income level.\n\n\n# income group vs access to credit\n# group individuals into quartiles by age\nquartiles = df_tc['person_income'].quantile([0.25, 0.5, 0.75])\nfirstQuartile = f\"{df_tc['person_income'].min()} to {quartiles[0.25]}\"\nsecondQuartile = f\"{quartiles[0.25]} to {quartiles[0.50]}\"\nthirdQuartile = f\"{quartiles[0.50]} to {quartiles[0.75]}\"\nfourthQuartile = f\"{quartiles[0.75]} to {df_tc['person_income'].max()}\"\n\n# separate individuals into quartiles\ndf_tc['income_quartile'] = pd.qcut(df_tc['person_income'], q = [0, 0.25, 0.5, 0.75, 1], labels = [firstQuartile, secondQuartile, thirdQuartile, fourthQuartile])\n\n# plot\nsns.boxplot(x=df_tc['income_quartile'], y=df_tc['probabilityPaid'])\nplt.title('Income Quartile vs Access to Credit')\nplt.ylabel('Score')\n\n/Users/mihirsingh/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n\n\nText(0, 0.5, 'Score')\n\n\n\n\n\n\n\n\n\nWe see that our model generally assigns higher scores to those with higher incomes."
  },
  {
    "objectID": "posts/decision-making/index.html#discussion",
    "href": "posts/decision-making/index.html#discussion",
    "title": "‘Optimal’ Decision-Making",
    "section": "Discussion",
    "text": "Discussion\nUltimately, I found that the threshold score was 0.53 and that the model was able to earn $7697437.56 in profit. I also found that the model tended to favor higher income individuals and was slightly biased against younger borrowers and borrowers seeking credit for medical reasons. We find that even with a simple formula for calculating bank profits, we were able to generate a large profit for the bank. From the bank’s perspective, I believe that the model satisfies a test of sufficiency since it seems that the the features the model looks at like loan intent and income level are relevant to whether someone will repay their loan. This seems fair from the bank’s perspective because things like medical debt are riskier to repay and thus should be harder to get approved. However, from the borrower’s perspective, this could be seen as unfair since people need to access credit for tremendous medical costs and it is not really their fault that they must get these treatments, so they are punished for something out of their control when they try to access credit. From this we see that what is fair and reasonable from one perspective may not be from another. With this in mind, one could argue that the model both satisfies and fails a narrow view of fairness since the bank might argue that with all things being equal - people are likely to get the same score. Meanwhile, a borrower might argue that loan-intent is not really a feature that should be used since people who are similar in every way except loan intent will probably receive different scores. It all from which perspective you evaluate the model and who you ask. I personally believe in a broader view of fairness that takes into account the implications of the model on the individual borrower and not just the bank. For this reason, I believe that the bank’s model is not fair since people will need to access credit for medical loans and they are unfairly punished for having such a need. Medical loan borrowers can’t control the fact that they need credit for medical expenses and thus should not be punished for it.\nFinally, I learned how to choose an optimal threshold for my model as well as how to investigate the results of that decision."
  },
  {
    "objectID": "posts/newton-optimizer/index.html",
    "href": "posts/newton-optimizer/index.html",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "Mihir Singh\n\n\nThe code for the Newton Optimizer can be found here.\n\n# imports\nfrom optimizer import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt\n\n\n\n\nIn this blog post, we will implement the Newton Optimizer for Logistic Regression. The Newton Optimizer is a second-order optimization algorithm that uses the Hessian matrix to find the optimal weights for the logistic regression model. We will implement the Newton Optimizer by adding to our previous implementation of logistic regression test it on a simple dataset to ensure that it behaves correctly. These experiments will include determining if the Newton Optimizer converges to the optimal weights giving a small alpha and if it converges faster than the Gradient Descent Optimizer. We will also check if it does not converge given a large alpha. Finally, we will calculate the computational cost of the Newton Optimizer and a Gradient Descent Optimizer’s before comparing and analyzing their costs.\n\n\n\nWith the Newton Optimizer implemented, we can perform some experiments to ensure that our implementation behaves correctly. We will use data from the previous logistic regression blog post.\n\n# Generating some data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.2)\n\nJust to be sure, we will plot our data to ensure that it is linearly separable.\n\n# function to plot data - from Professor Phil Chodrow\ndef plot_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# function to plot data - from Professor Phil Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n    \n# function to determine accuracy of logistic regression models\ndef accuracy(model, X, y):\n    return torch.mean((1.0*(model.predict(X) == y)))\n    \n# plotting loss\ndef loss_graph(loss_vec):\n    plt.plot(loss_vec, marker='o', linestyle='-')\n    plt.title('Loss Graph: Experiment 1')\n    plt.xlabel('Iteration')\n    plt.ylabel('Loss')\n    plt.show()\n    print(\"final loss: \", loss_vec[len(loss_vec)-1])\n\nfig, ax = plt.subplots(1, 1)\n\nX, y = classification_data()\nplot_data(X, y, ax)\n\n\n\n\n\n\n\n\nIndeed, the data seems, at least visually, to be linearly separable.\n\n\nWe will first make sure that when alpha is chosen appropriately, the Newton Optimizer converges to the optimal solution. We will first make our model then run it over a training loop before evaluating its performance.\n\n# creating our model\nLR = LogisticRegression()\nopt = NewtonOptimizer(LR)\n\n# training loop\nlossVec = []\nfor _ in range(7500):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = opt.step(X, y, alpha = 0.25)\n    lossVec.append(loss)\n    \n# plot decision classification\nfix, ax = plt.subplots(1, 1)\nplot_data(X, y, ax)\ndraw_line(LR.w, X[:,0].min(), X[:,0].max(), ax, color = \"black\")\n\n# determine accuracy\naccuracy(LR, X, y)\n\ntensor(1.)\n\n\n\n\n\n\n\n\n\nWe see that our model does indeed converge to the optimal solution, with an accuracy of 100%.\nWe can also look at a loss graph to see how the model comes to a decision.\n\nloss_graph(lossVec)\n\n\n\n\n\n\n\n\nfinal loss:  tensor(0.0011)\n\n\nWe see that our loss converges to essentially 0 over the course of 7500 iterations with an alpha of 0.25.\n\n\n\nWe will now see if Newton’s method can converge much faster than standard gradient descent in terms of the empirical risk. We will try to see this behavior by making the alpha for the Newton Optimizer much larger.\n\n# creating our model\nLRnewt = LogisticRegression()\nLRgrad = LogisticRegression()\nnewtonOpt = NewtonOptimizer(LRnewt)\ngradOpt = GradientDescentOptimizer(LRgrad)\n\ngradLossVec = []\nfor _ in range(7500):\n    gradLoss = gradOpt.step(X, y, alpha = 0.01, beta = 0)\n    gradLossVec.append(gradLoss)\n\nnewtLossVec = []\nfor _ in range(7500):\n    newtLoss = newtonOpt.step(X, y, alpha = 0.4)\n    newtLossVec.append(newtLoss)\n    \n\nAfter training the models, we can plot their losses.\n\nplt.plot(newtLossVec, label=\"Newton's Method\")\nplt.plot(gradLossVec, label=\"Gradient Descent\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Steo\")\nplt.title(\"Newton's Method vs. Gradient Descent Convergece\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe see here that when the Newton Optimizer’s alpha is much larger, and the standard gradient descent optimizer’s alpha is very small, we have a scenario where the Newton Optimizer converges much faster than the standard gradient descent optimizer.\n\n\n\nWe will now look at a situation where the alpha provided to the Newton Optimizer is very large. When the alpha is too large, the Newton Optimizer will not converge to the optimal solution.\n\nX, y = classification_data(n_points=300, noise = 0.5)\n\n# create Newton Optimizer\nLRnewt = LogisticRegression()\nnewtonOpt = NewtonOptimizer(LRnewt)\n\n# run training loop\nnewtLossVec = []\nfor _ in range(100):\n    newtLoss = newtonOpt.step(X, y, alpha = 610)\n    newtLossVec.append(newtLoss)\n\nAfter we have run our training loop, we can look at the loss graph.\n\nloss_graph(newtLossVec)\n\n\n\n\n\n\n\n\nfinal loss:  tensor(0.2021)\n\n\nWe can see that when our alpha gets very large, we actually end up having our loss increase over time, thus with a high alpha, our model does not converge.\n\n\n\n\nAssumptions: 1. Compute loss L costs C units. 2. Compute gradient G costs 2C units. 3. Compute Hessian H costs pc units. 4. Inverting pxp matrix is \\(k_1p^\\gamma\\) units 5. Multiplying by hessian = \\(k_2p^\\gamma\\)\nWe also assume that Newton’s method converges to an adequate solution in \\(t_\\mathrm{nm}\\) steps and standard gradient descent converges in \\(t_\\mathrm{gd}\\) steps.\nNow to calculate the cost of Newton’s method:\nsingle step cost: (2c + pc + \\(k_1p^\\gamma\\) + $ \\(k_2p^\\gamma\\))\nsum costs: c + \\(t_\\mathrm{nm}\\) + (2c + pc + \\(k_1p^\\gamma\\) + $ \\(k_2p^\\gamma\\))\nNow to calculate the cost of standard method:\nsingle step cost: c\nsum costs: c + \\(t_\\mathrm{gd}\\) x 2c\nWith these costs calculated, we can compare the two methods. It seems that generally, Newton’s method is more computationally expensive, but to compare we can create an inequality where the standard gradient descent cost is more than the newton method cost:\nO(standard) &gt; O(newton)\nc + \\(t_\\mathrm{gd}\\) x 2c &gt; c + \\(t_\\mathrm{nm}\\) + (2c + pc + \\(k_1p^\\gamma\\) + $ \\(k_2p^\\gamma\\))\nwith some simplification, we get\n\\(t_\\mathrm{gd}\\) &gt; \\(t_\\mathrm{nm}\\) * (1 + p/2 + \\(k_1p^\\gamma\\)/2c + \\(k_2p^\\gamma\\)/2c)\nSo in order for Newton’s method to be more computationally efficient, the above inequality must hold and \\(t_\\mathrm{gd}\\) must be larger than \\(t_\\mathrm{nm}\\) by a factor of (1 + p/2 + \\(k_1p^\\gamma\\)/2c + \\(k_2p^\\gamma\\)/2c)\nIf p became very large, Newton’s method would not every pay off since there are factors affecting the computational cost of the Newton Method that are dependent on, and grow polynomially with p. This means the cost of Newton’s method grows significantly when p or \\(\\gamma\\) increases. With larger problems, standard gradient descent is probably a better optimization choice.\n\n\n\nIn this blog post, I implemented Newton’s method for optimization and conducted a variety of experiments that tested its behavior. We saw that with a small enough alpha, Newton’s method can converge, but that it takes a very specific set of circumstances (namely that Newton’s alpha be very large and standard gradient descent’s alpha be much smaller) for it to converge faster than standard gradient descent. We also saw that when Newton’s alpha is too large, the model does not converge. Finally, we calculated the computation cost of Newton’s method and found that it is generally more computationally expensive than standard gradient descent, but that it can be more efficient in certain, rare, scenarios."
  },
  {
    "objectID": "posts/newton-optimizer/index.html#implementing-newton-optimizer",
    "href": "posts/newton-optimizer/index.html#implementing-newton-optimizer",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "The code for the Newton Optimizer can be found here.\n\n# imports\nfrom optimizer import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer\nimport torch\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/newton-optimizer/index.html#experiments",
    "href": "posts/newton-optimizer/index.html#experiments",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "With the Newton Optimizer implemented, we can perform some experiments to ensure that our implementation behaves correctly. We will use data from the previous logistic regression blog post.\n\n# Generating some data\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.2)\n\nJust to be sure, we will plot our data to ensure that it is linearly separable.\n\n# function to plot data - from Professor Phil Chodrow\ndef plot_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n# function to plot data - from Professor Phil Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n    \n# function to determine accuracy of logistic regression models\ndef accuracy(model, X, y):\n    return torch.mean((1.0*(model.predict(X) == y)))\n    \n# plotting loss\ndef loss_graph(loss_vec):\n    plt.plot(loss_vec, marker='o', linestyle='-')\n    plt.title('Loss Graph: Experiment 1')\n    plt.xlabel('Iteration')\n    plt.ylabel('Loss')\n    plt.show()\n    print(\"final loss: \", loss_vec[len(loss_vec)-1])\n\nfig, ax = plt.subplots(1, 1)\n\nX, y = classification_data()\nplot_data(X, y, ax)\n\n\n\n\n\n\n\n\nIndeed, the data seems, at least visually, to be linearly separable.\n\n\nWe will first make sure that when alpha is chosen appropriately, the Newton Optimizer converges to the optimal solution. We will first make our model then run it over a training loop before evaluating its performance.\n\n# creating our model\nLR = LogisticRegression()\nopt = NewtonOptimizer(LR)\n\n# training loop\nlossVec = []\nfor _ in range(7500):\n    # add other stuff to e.g. keep track of the loss over time. \n    loss = opt.step(X, y, alpha = 0.25)\n    lossVec.append(loss)\n    \n# plot decision classification\nfix, ax = plt.subplots(1, 1)\nplot_data(X, y, ax)\ndraw_line(LR.w, X[:,0].min(), X[:,0].max(), ax, color = \"black\")\n\n# determine accuracy\naccuracy(LR, X, y)\n\ntensor(1.)\n\n\n\n\n\n\n\n\n\nWe see that our model does indeed converge to the optimal solution, with an accuracy of 100%.\nWe can also look at a loss graph to see how the model comes to a decision.\n\nloss_graph(lossVec)\n\n\n\n\n\n\n\n\nfinal loss:  tensor(0.0011)\n\n\nWe see that our loss converges to essentially 0 over the course of 7500 iterations with an alpha of 0.25.\n\n\n\nWe will now see if Newton’s method can converge much faster than standard gradient descent in terms of the empirical risk. We will try to see this behavior by making the alpha for the Newton Optimizer much larger.\n\n# creating our model\nLRnewt = LogisticRegression()\nLRgrad = LogisticRegression()\nnewtonOpt = NewtonOptimizer(LRnewt)\ngradOpt = GradientDescentOptimizer(LRgrad)\n\ngradLossVec = []\nfor _ in range(7500):\n    gradLoss = gradOpt.step(X, y, alpha = 0.01, beta = 0)\n    gradLossVec.append(gradLoss)\n\nnewtLossVec = []\nfor _ in range(7500):\n    newtLoss = newtonOpt.step(X, y, alpha = 0.4)\n    newtLossVec.append(newtLoss)\n    \n\nAfter training the models, we can plot their losses.\n\nplt.plot(newtLossVec, label=\"Newton's Method\")\nplt.plot(gradLossVec, label=\"Gradient Descent\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Steo\")\nplt.title(\"Newton's Method vs. Gradient Descent Convergece\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nWe see here that when the Newton Optimizer’s alpha is much larger, and the standard gradient descent optimizer’s alpha is very small, we have a scenario where the Newton Optimizer converges much faster than the standard gradient descent optimizer.\n\n\n\nWe will now look at a situation where the alpha provided to the Newton Optimizer is very large. When the alpha is too large, the Newton Optimizer will not converge to the optimal solution.\n\nX, y = classification_data(n_points=300, noise = 0.5)\n\n# create Newton Optimizer\nLRnewt = LogisticRegression()\nnewtonOpt = NewtonOptimizer(LRnewt)\n\n# run training loop\nnewtLossVec = []\nfor _ in range(100):\n    newtLoss = newtonOpt.step(X, y, alpha = 610)\n    newtLossVec.append(newtLoss)\n\nAfter we have run our training loop, we can look at the loss graph.\n\nloss_graph(newtLossVec)\n\n\n\n\n\n\n\n\nfinal loss:  tensor(0.2021)\n\n\nWe can see that when our alpha gets very large, we actually end up having our loss increase over time, thus with a high alpha, our model does not converge."
  },
  {
    "objectID": "posts/newton-optimizer/index.html#operation-counting",
    "href": "posts/newton-optimizer/index.html#operation-counting",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "Assumptions: 1. Compute loss L costs C units. 2. Compute gradient G costs 2C units. 3. Compute Hessian H costs pc units. 4. Inverting pxp matrix is \\(k_1p^\\gamma\\) units 5. Multiplying by hessian = \\(k_2p^\\gamma\\)\nWe also assume that Newton’s method converges to an adequate solution in \\(t_\\mathrm{nm}\\) steps and standard gradient descent converges in \\(t_\\mathrm{gd}\\) steps.\nNow to calculate the cost of Newton’s method:\nsingle step cost: (2c + pc + \\(k_1p^\\gamma\\) + $ \\(k_2p^\\gamma\\))\nsum costs: c + \\(t_\\mathrm{nm}\\) + (2c + pc + \\(k_1p^\\gamma\\) + $ \\(k_2p^\\gamma\\))\nNow to calculate the cost of standard method:\nsingle step cost: c\nsum costs: c + \\(t_\\mathrm{gd}\\) x 2c\nWith these costs calculated, we can compare the two methods. It seems that generally, Newton’s method is more computationally expensive, but to compare we can create an inequality where the standard gradient descent cost is more than the newton method cost:\nO(standard) &gt; O(newton)\nc + \\(t_\\mathrm{gd}\\) x 2c &gt; c + \\(t_\\mathrm{nm}\\) + (2c + pc + \\(k_1p^\\gamma\\) + $ \\(k_2p^\\gamma\\))\nwith some simplification, we get\n\\(t_\\mathrm{gd}\\) &gt; \\(t_\\mathrm{nm}\\) * (1 + p/2 + \\(k_1p^\\gamma\\)/2c + \\(k_2p^\\gamma\\)/2c)\nSo in order for Newton’s method to be more computationally efficient, the above inequality must hold and \\(t_\\mathrm{gd}\\) must be larger than \\(t_\\mathrm{nm}\\) by a factor of (1 + p/2 + \\(k_1p^\\gamma\\)/2c + \\(k_2p^\\gamma\\)/2c)\nIf p became very large, Newton’s method would not every pay off since there are factors affecting the computational cost of the Newton Method that are dependent on, and grow polynomially with p. This means the cost of Newton’s method grows significantly when p or \\(\\gamma\\) increases. With larger problems, standard gradient descent is probably a better optimization choice."
  },
  {
    "objectID": "posts/newton-optimizer/index.html#discussion",
    "href": "posts/newton-optimizer/index.html#discussion",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In this blog post, I implemented Newton’s method for optimization and conducted a variety of experiments that tested its behavior. We saw that with a small enough alpha, Newton’s method can converge, but that it takes a very specific set of circumstances (namely that Newton’s alpha be very large and standard gradient descent’s alpha be much smaller) for it to converge faster than standard gradient descent. We also saw that when Newton’s alpha is too large, the model does not converge. Finally, we calculated the computation cost of Newton’s method and found that it is generally more computationally expensive than standard gradient descent, but that it can be more efficient in certain, rare, scenarios."
  },
  {
    "objectID": "posts/newton-optimizer/index.html#abstract",
    "href": "posts/newton-optimizer/index.html#abstract",
    "title": "Newton’s Method for Logistic Regression",
    "section": "",
    "text": "In this blog post, we will implement the Newton Optimizer for Logistic Regression. The Newton Optimizer is a second-order optimization algorithm that uses the Hessian matrix to find the optimal weights for the logistic regression model. We will implement the Newton Optimizer by adding to our previous implementation of logistic regression test it on a simple dataset to ensure that it behaves correctly. These experiments will include determining if the Newton Optimizer converges to the optimal weights giving a small alpha and if it converges faster than the Gradient Descent Optimizer. We will also check if it does not converge given a large alpha. Finally, we will calculate the computational cost of the Newton Optimizer and a Gradient Descent Optimizer’s before comparing and analyzing their costs."
  }
]
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Mihir Singh"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Mihir Singh\n\n\nBy developing a model to classify the Palmer penguins, there can be a more accurate species identification of penguins given a certain set of features. Comparing the different features in the Palmer penguin data set, it was determined that the culmen length, culmen depth, and island are useful features for determining a penguin’s species. Different types of classification models were also compared (logistic regress, decision tree, and SVC). It was determined that a logistic regression model looking at a penguins island of residence, culmen depth, and culmen depth would be a highly accurate model. When it was applied to the testing data, it received an accuracy score of %100.\nFirst we must access the training data. We can also import any libraries we might need later on.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nThis code prepares qualitative columns in the data. Provided by Professor Phil Chodrow.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNo we can then create visualizations that give us some new understanding about the Palmer Penguins.\nWe can first look to see if there is any difference in body mass by island. To see this, we can create a box plot with the body mass being on the y-axis and the islands being the discrete categories making up the x-axis.\n\np1 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", )\n\n\n\n\n\n\n\n\nIn p1 we can see that penguins on Biscoe island are generally much larger in body mass than penguins on other islands.\nWe can then also look to see if the difference is holds within species. That is, is a Chinstrap penguin on Biscoe island generally bigger than a Chinstrap penguin on Torgersen island, or is it that a certain species of penguin dominates Biscoe island.\n\np2 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = \"Species\" ,palette=[\"r\", \"g\", 'b'])\n\n\n\n\n\n\n\n\nIn p2 we find that a reason that the penguins on Biscoe Island are heavier may be because of its population of Gentoo Penguins. There are no other islands with a Gentoo penguin population, and they seem to be much heavier than other penguin species.\nWe can check this by looking at the mean body mass of penguins across species.\n\nsummaryTable = train.groupby(['Species']).aggregate(avgMass=('Body Mass (g)', 'mean'))\nprint(summaryTable)\n\n                                               avgMass\nSpecies                                               \nAdelie Penguin (Pygoscelis adeliae)        3718.487395\nChinstrap penguin (Pygoscelis antarctica)  3743.421053\nGentoo penguin (Pygoscelis papua)          5039.948454\n\n\nIndeed, we see that Gentoo penguins are, on average, much heavier than other penguins. So it makes sense to conclude that their prescence on Biscoe Island and abscence on other islands might be a big reason the average penguin on Biscoe Island is larger than the average penguin on other islands.\nIt might also be interesting to see if penguins with a bigger culmen length or depth are heavier. Again we can plot body mass and culmen length/depth to see this relationship.\n\np3 = sns.boxplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\np3 = sns.boxplot(train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\")\np3.set_xticks([0,10,20,30,40,50,60,70])\n\n\n\n\n\n\n\n\nWe see that as culmen length increases, the mass of the penguin generally increases, but the same is not true for culmen depth. In fact, the body mass is highest when the culmen depth reaches around 15-16 mm. These measurements line up with the culmen depth range of Chinstrap penguins so we can conclude that culmen depth doesn’t really have that much of an effect on body mass.\n\n\n\nFrom our visualizations we can infer that body mass and the island the penguin is on might be useful features. However, we should probably look for 3 good features.\nWe can start by eliminating features with low variance.\n\nfrom sklearn.feature_selection import VarianceThreshold\nX = X_train\n\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nnewTraining = sel.fit_transform(X)\n\nFirst, we can iterate through every qualitative and quantitative variable and create different combinations of features to test.\nThen we can start preparing models. The models I will use are a logistic regression, decision tree, and svc classifier.\nFinally, we can cross-validate these models.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import cross_val_score\n\n# values to keep track of best score from each classifier\nLRBest = 0\nDTBest = 0\nSVCBest = 0\nLRBestCols = []\nDTBestCols = []\nSVCBestCols = []\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    \n    # logistic regression\n    LR = LogisticRegression(max_iter=5000)\n    LR.fit(X_train[cols], y_train)\n    scoreLR = LR.score(X_train[cols], y_train)\n    if scoreLR &gt; LRBest:\n      LRBest = scoreLR\n      LRBestCols = cols\n    \n    # decision tree\n    DT = DecisionTreeClassifier(max_depth=20)\n    DT.fit(X_train[cols], y_train)\n    scoreDT = DT.score(X_train[cols], y_train)\n    if scoreDT &gt; DTBest:\n      DTBest = scoreDT\n      DTBestCols = cols\n    \n    # SVC\n    SVCModel = SVC(gamma='auto')\n    SVCModel.fit(X_train[cols], y_train)\n    scoreSVC = SVCModel.score(X_train[cols], y_train)\n    if scoreSVC &gt; SVCBest:\n      SVCBest = scoreSVC\n      SVCBestCols = cols\n\nprint(LRBest, \" \", LRBestCols)\nprint(DTBest, \" \", DTBestCols)\nprint(SVCBest, \" \", SVCBestCols)\n\n# cross validate models\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\nprint((cv_scores_LR).mean())\n\ncv_scores_DT = cross_val_score(DT, X_train, y_train, cv=5)\nprint((cv_scores_DT).mean())\n\ncv_scores_SVC = cross_val_score(SVCModel, X_train, y_train, cv=5)\nprint((cv_scores_SVC).mean())\n\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0   ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n0.9569381598793363\n0.6407239819004525\n\n\nWhile we do find that the decision tree classifier does have a After cross-validating the models, it seems that a logistic regression is best. We can now look at creating a logistic regression model with the three best features for it (which were Island, Culmen Length, and Culmen Depth).\nWe found these three best features by running through every possible combination of three features (with one qualitative and 2 quantitative) and keep track of which combination led to the high model score.\n\n\n\nWith our models and features selected, we can test our decisions/model.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\n\n\n\nWe can plot the decision regions of our classifiers to get a better understanding of the conclusions our model drew.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \nplot_regions(LR, X_train[cols], y_train)\n\n\n\n\n\n\n\n\n\n\n\nWe can also take a look at a confusion matrix to give us a better understanding of our model’s accuracy.\n\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = LR.predict(X_test[cols])\ny_test_pred\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nFrom this we see that we correctly predict every penguin. Since there are no values outside the diagonal of the matrix, we don’t have any false predictions. Given that we use island as a feature, we will probably see more errors predicting Adelie penguins as Chinstrap or Gentoo. This is because the Adelie penguin is the only penguin species that is present on every island so it is probably easiest for our model to confuse it for another species.\n\n\n\nA logistic regression model trained by looking at the Palmer penguin dataset features of culmen length, culmen depth, and home island was highly accurate, getting an accuracy score of %100 on the test data.\nBy finding this model, I learned about the importance of conducting thorough research on your data. Much of the initial work did not involve creating models but exploring the features and characteristics of the dataset. Only after this exploration was I able to develop an intuition for what features might be useful in creating a model. However, just exploring the data is not enough to create a good model, cross-validation and eliminating features with low variance were also important techniques employed in this process, guiding against overfitting and helping narrow down on useful features respectively."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#abstract",
    "href": "posts/classifying-palmer-penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "By developing a model to classify the Palmer penguins, there can be a more accurate species identification of penguins given a certain set of features. Comparing the different features in the Palmer penguin data set, it was determined that the culmen length, culmen depth, and island are useful features for determining a penguin’s species. Different types of classification models were also compared (logistic regress, decision tree, and SVC). It was determined that a logistic regression model looking at a penguins island of residence, culmen depth, and culmen depth would be a highly accurate model. When it was applied to the testing data, it received an accuracy score of %100.\nFirst we must access the training data. We can also import any libraries we might need later on.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#data-preparation",
    "href": "posts/classifying-palmer-penguins/index.html#data-preparation",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "This code prepares qualitative columns in the data. Provided by Professor Phil Chodrow.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#explore",
    "href": "posts/classifying-palmer-penguins/index.html#explore",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "No we can then create visualizations that give us some new understanding about the Palmer Penguins.\nWe can first look to see if there is any difference in body mass by island. To see this, we can create a box plot with the body mass being on the y-axis and the islands being the discrete categories making up the x-axis.\n\np1 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", )\n\n\n\n\n\n\n\n\nIn p1 we can see that penguins on Biscoe island are generally much larger in body mass than penguins on other islands.\nWe can then also look to see if the difference is holds within species. That is, is a Chinstrap penguin on Biscoe island generally bigger than a Chinstrap penguin on Torgersen island, or is it that a certain species of penguin dominates Biscoe island.\n\np2 = sns.boxplot(train, x = \"Island\", y = \"Body Mass (g)\", hue = \"Species\" ,palette=[\"r\", \"g\", 'b'])\n\n\n\n\n\n\n\n\nIn p2 we find that a reason that the penguins on Biscoe Island are heavier may be because of its population of Gentoo Penguins. There are no other islands with a Gentoo penguin population, and they seem to be much heavier than other penguin species.\nWe can check this by looking at the mean body mass of penguins across species.\n\nsummaryTable = train.groupby(['Species']).aggregate(avgMass=('Body Mass (g)', 'mean'))\nprint(summaryTable)\n\n                                               avgMass\nSpecies                                               \nAdelie Penguin (Pygoscelis adeliae)        3718.487395\nChinstrap penguin (Pygoscelis antarctica)  3743.421053\nGentoo penguin (Pygoscelis papua)          5039.948454\n\n\nIndeed, we see that Gentoo penguins are, on average, much heavier than other penguins. So it makes sense to conclude that their prescence on Biscoe Island and abscence on other islands might be a big reason the average penguin on Biscoe Island is larger than the average penguin on other islands.\nIt might also be interesting to see if penguins with a bigger culmen length or depth are heavier. Again we can plot body mass and culmen length/depth to see this relationship.\n\np3 = sns.boxplot(train, x = \"Culmen Length (mm)\", y = \"Body Mass (g)\")\n\n\n\n\n\n\n\n\n\np3 = sns.boxplot(train, x = \"Culmen Depth (mm)\", y = \"Body Mass (g)\")\np3.set_xticks([0,10,20,30,40,50,60,70])\n\n\n\n\n\n\n\n\nWe see that as culmen length increases, the mass of the penguin generally increases, but the same is not true for culmen depth. In fact, the body mass is highest when the culmen depth reaches around 15-16 mm. These measurements line up with the culmen depth range of Chinstrap penguins so we can conclude that culmen depth doesn’t really have that much of an effect on body mass."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#choosing-features-and-models",
    "href": "posts/classifying-palmer-penguins/index.html#choosing-features-and-models",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "From our visualizations we can infer that body mass and the island the penguin is on might be useful features. However, we should probably look for 3 good features.\nWe can start by eliminating features with low variance.\n\nfrom sklearn.feature_selection import VarianceThreshold\nX = X_train\n\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nnewTraining = sel.fit_transform(X)\n\nFirst, we can iterate through every qualitative and quantitative variable and create different combinations of features to test.\nThen we can start preparing models. The models I will use are a logistic regression, decision tree, and svc classifier.\nFinally, we can cross-validate these models.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import cross_val_score\n\n# values to keep track of best score from each classifier\nLRBest = 0\nDTBest = 0\nSVCBest = 0\nLRBestCols = []\nDTBestCols = []\nSVCBestCols = []\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    \n    # logistic regression\n    LR = LogisticRegression(max_iter=5000)\n    LR.fit(X_train[cols], y_train)\n    scoreLR = LR.score(X_train[cols], y_train)\n    if scoreLR &gt; LRBest:\n      LRBest = scoreLR\n      LRBestCols = cols\n    \n    # decision tree\n    DT = DecisionTreeClassifier(max_depth=20)\n    DT.fit(X_train[cols], y_train)\n    scoreDT = DT.score(X_train[cols], y_train)\n    if scoreDT &gt; DTBest:\n      DTBest = scoreDT\n      DTBestCols = cols\n    \n    # SVC\n    SVCModel = SVC(gamma='auto')\n    SVCModel.fit(X_train[cols], y_train)\n    scoreSVC = SVCModel.score(X_train[cols], y_train)\n    if scoreSVC &gt; SVCBest:\n      SVCBest = scoreSVC\n      SVCBestCols = cols\n\nprint(LRBest, \" \", LRBestCols)\nprint(DTBest, \" \", DTBestCols)\nprint(SVCBest, \" \", SVCBestCols)\n\n# cross validate models\ncv_scores_LR = cross_val_score(LR, X_train, y_train, cv=5)\nprint((cv_scores_LR).mean())\n\ncv_scores_DT = cross_val_score(DT, X_train, y_train, cv=5)\nprint((cv_scores_DT).mean())\n\ncv_scores_SVC = cross_val_score(SVCModel, X_train, y_train, cv=5)\nprint((cv_scores_SVC).mean())\n\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0   ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n0.99609375   ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n1.0\n0.9569381598793363\n0.6407239819004525\n\n\nWhile we do find that the decision tree classifier does have a After cross-validating the models, it seems that a logistic regression is best. We can now look at creating a logistic regression model with the three best features for it (which were Island, Culmen Length, and Culmen Depth).\nWe found these three best features by running through every possible combination of three features (with one qualitative and 2 quantitative) and keep track of which combination led to the high model score."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#testing",
    "href": "posts/classifying-palmer-penguins/index.html#testing",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "With our models and features selected, we can test our decisions/model.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ncols = ['Culmen Length (mm)', 'Culmen Depth (mm)','Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test)\n\n1.0"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "We can plot the decision regions of our classifiers to get a better understanding of the conclusions our model drew.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n      \nplot_regions(LR, X_train[cols], y_train)"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#confusion-matrix",
    "href": "posts/classifying-palmer-penguins/index.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "We can also take a look at a confusion matrix to give us a better understanding of our model’s accuracy.\n\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = LR.predict(X_test[cols])\ny_test_pred\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 11,  0],\n       [ 0,  0, 26]])\n\n\nFrom this we see that we correctly predict every penguin. Since there are no values outside the diagonal of the matrix, we don’t have any false predictions. Given that we use island as a feature, we will probably see more errors predicting Adelie penguins as Chinstrap or Gentoo. This is because the Adelie penguin is the only penguin species that is present on every island so it is probably easiest for our model to confuse it for another species."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#discussion",
    "href": "posts/classifying-palmer-penguins/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "A logistic regression model trained by looking at the Palmer penguin dataset features of culmen length, culmen depth, and home island was highly accurate, getting an accuracy score of %100 on the test data.\nBy finding this model, I learned about the importance of conducting thorough research on your data. Much of the initial work did not involve creating models but exploring the features and characteristics of the dataset. Only after this exploration was I able to develop an intuition for what features might be useful in creating a model. However, just exploring the data is not enough to create a good model, cross-validation and eliminating features with low variance were also important techniques employed in this process, guiding against overfitting and helping narrow down on useful features respectively."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]